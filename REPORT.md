# Do LLMs Differentiate Epistemic Belief from Non-Epistemic Belief?

## 1. Executive Summary

**Research Question**: Do frontier large language models exhibit measurable differentiation between epistemic beliefs (factual claims about the world) and non-epistemic beliefs (values, preferences, opinions)?

**Key Finding**: Yes — frontier LLMs strongly differentiate epistemic from non-epistemic beliefs across three independent measures: classification accuracy (97.5–99.0%), confidence calibration (Cohen's d = 5.0–7.3), and reasoning signature analysis (Cramér's V = 0.38–0.56). This contrasts sharply with prior work using small open-weight models (Qwen2.5-0.5B), which showed no differentiation, suggesting that the epistemic/non-epistemic distinction is an emergent capability of larger models.

**Practical Implications**: Frontier LLMs can reliably distinguish facts from opinions and articulate appropriate reasoning for each, but they systematically classify empirically-testable value claims as facts — revealing that their notion of "epistemic" is grounded in testability rather than the human intuition that guides opinion labels.

---

## 2. Goal

### Hypothesis
Large language models exhibit differentiation between epistemic beliefs and non-epistemic beliefs, analogous to distinctions observed in human cognition.

### Why This Is Important
Vesga et al. argue that humans maintain fundamentally different types of beliefs with distinct cognitive dynamics. If LLMs similarly differentiate, this has implications for:
- **AI safety**: Models that conflate facts and opinions may present subjective claims as established truths
- **Alignment**: Understanding how models represent belief types informs how we should design value-sensitive systems
- **Deployment**: Applications in education, journalism, and law require systems that can flag opinion vs. fact

### Sub-Hypotheses
- **H1 (Classification)**: Frontier LLMs classify fact vs. opinion significantly above chance (50%)
- **H2 (Calibration)**: Models assign systematically different confidence levels to facts vs. opinions (Cohen's d > 0.5)
- **H3 (Reasoning Signatures)**: Model rationales exhibit different linguistic patterns for facts vs. opinions (Cramér's V > 0.1)

---

## 3. Data Construction

### Dataset Description
**Primary dataset**: `agentlans/fact-or-opinion` (HuggingFace, 2025)
- 41,549 statements across 15 languages, synthetically generated
- 4 labels: Fact, Opinion, Both, Neither
- Generated by ChatGPT, Claude Sonnet 4, Gemini 2.5 Flash, DeepSeek, Microsoft Copilot, Le Chat, Perplexity
- License: ODC-BY

**Filtered to English**: 2,220 training / 555 test statements

### Example Samples

| Statement | True Label |
|-----------|-----------|
| "The sum of angles in a triangle is 180 degrees." | Fact |
| "The sun is approximately 93 million miles from Earth." | Fact |
| "Owning a house is more satisfying than renting." | Opinion |
| "Silence is more powerful than words." | Opinion |
| "New York City has more than 8 million residents and its pace can be exhilarating or exhausting." | Both |
| "Imagine we could live forever." | Neither |

### Data Quality
- No missing values in text or label fields
- Statements are concise (mean ~47 characters, ~8 tokens)
- Synthetic origin means potential artifacts from generating LLMs
- Label distribution in English test set: Opinion 147, Both 146, Neither 141, Fact 121

### Preprocessing Steps
1. Loaded from HuggingFace Arrow format
2. Filtered to English-language entries (`language == 'en'`)
3. For primary experiment: filtered to Fact and Opinion labels only
4. Sampled balanced evaluation set: 100 Fact + 100 Opinion (random seed=42)
5. Created separate exploratory set: 25 Both + 25 Neither

### Train/Val/Test Splits
- **Training set** (for TF-IDF baseline): 1,074 English statements (485 Fact, 589 Opinion)
- **Evaluation set** (all models): 200 statements (100 Fact, 100 Opinion)
- **Exploratory set**: 50 statements (25 Both, 25 Neither) — tested with GPT-4.1 only

---

## 4. Experiment Description

### Methodology

#### High-Level Approach
We test three frontier OpenAI models and a TF-IDF baseline on binary fact/opinion classification. Each LLM receives a zero-shot prompt requesting structured JSON output with classification label, confidence score, and reasoning rationale. We analyze three complementary dimensions: explicit classification (H1), implicit confidence differentiation (H2), and reasoning vocabulary analysis (H3).

#### Why This Method?
- **Zero-shot prompting**: Avoids confounding effects of in-context examples; tests inherent model capability
- **Structured JSON output**: Enables quantitative analysis of confidence and reasoning without manual annotation
- **Multiple models at different sizes**: Tests whether differentiation scales with model capability
- **TF-IDF baseline**: Establishes that the dataset itself is separable, isolating the question of whether LLMs exploit the right features

### Implementation Details

#### Tools and Libraries
| Library | Version | Purpose |
|---------|---------|---------|
| Python | 3.12.8 | Runtime |
| openai | 2.24.0 | GPT API calls |
| scikit-learn | 1.8.0 | TF-IDF, LogReg, metrics |
| scipy | 1.17.1 | Statistical tests |
| matplotlib | 3.10.8 | Visualizations |
| numpy | latest | Numerical operations |

#### Models Tested
| Model | Type | Parameters | API |
|-------|------|-----------|-----|
| TF-IDF + LogReg | Non-neural baseline | ~5K features | Local |
| GPT-4o-mini | Small frontier | Unknown | OpenAI |
| GPT-4o | Large frontier | Unknown | OpenAI |
| GPT-4.1 | Latest frontier | Unknown | OpenAI |

#### Prompt Design

**System prompt**:
> You are an analyst who distinguishes between epistemic beliefs and non-epistemic beliefs. An epistemic belief (Fact) is a truth-evaluable proposition about the world — something that can be verified or falsified through evidence, observation, or logical reasoning. A non-epistemic belief (Opinion) is a value judgment, preference, or subjective evaluation that cannot be definitively proven true or false. You must return ONLY valid JSON with no other text.

**User prompt** (per statement):
> Classify whether the following statement expresses an epistemic belief (Fact) or a non-epistemic belief (Opinion). Statement: "{statement}". Respond with ONLY a JSON object with keys: "label" (Fact/Opinion), "confidence" (0-1 probability that it IS a Fact), "rationale" (1-2 sentence explanation).

#### Hyperparameters
| Parameter | Value | Justification |
|-----------|-------|---------------|
| Temperature | 0 | Deterministic output for reproducibility |
| Max tokens | 200 | Sufficient for JSON response |
| Random seed | 42 | Standard reproducibility seed |
| TF-IDF n-gram range | (1, 2) | Captures unigrams and bigrams |
| TF-IDF max features | 5,000 | Adequate for small vocabulary |
| LogReg class weight | Balanced | Handles slight class imbalance |

#### Reproducibility Information
- **Random seed**: 42 (set for numpy, random, sklearn)
- **API determinism**: Temperature=0 for all API calls
- **Models**: GPT-4.1, GPT-4o, GPT-4o-mini (OpenAI, February 2026)
- **Hardware**: CPU-only (no GPU available)
- **Parse success rate**: 100% across all models (200/200 statements)
- **Total API calls**: 600 (3 models × 200 statements) + 50 (exploratory)

#### Evaluation Metrics
- **Accuracy** and **Macro-F1**: Classification performance
- **Brier Score**: Calibration quality (lower = better calibrated)
- **Cohen's d**: Effect size for confidence gap between fact and opinion
- **Cramér's V**: Association strength between belief type and reasoning vocabulary
- **Binomial test**: Whether accuracy exceeds 50% chance
- **Welch's t-test**: Whether confidence distributions differ significantly
- **Chi-square / Fisher's exact test**: Whether keyword distributions differ by belief type

### Raw Results

#### H1: Classification Performance

| Model | Accuracy | 95% CI | Macro-F1 | Fact P/R | Opinion P/R |
|-------|----------|--------|----------|----------|-------------|
| TF-IDF Baseline | 0.975 | [0.943, 0.989] | 0.975 | 0.970/0.980 | 0.980/0.970 |
| GPT-4.1 | 0.975 | [0.943, 0.989] | 0.975 | 0.952/1.000 | 1.000/0.950 |
| GPT-4o | 0.980 | [0.950, 0.992] | 0.980 | 0.962/1.000 | 1.000/0.960 |
| GPT-4o-mini | **0.990** | [0.964, 0.997] | **0.990** | 1.000/0.980 | 0.980/1.000 |

All models: binomial test p ≈ 0 (all significantly above 50% chance).

#### H2: Confidence Calibration

| Model | Fact Conf (μ±σ) | Opinion Conf (μ±σ) | Difference | Cohen's d | Brier Score | p-value |
|-------|----------------|-------------------|------------|-----------|-------------|---------|
| TF-IDF Baseline | 0.785 ± 0.137 | 0.244 ± 0.131 | 0.541 | 4.01 | 0.071 | < 10⁻⁷⁰ |
| GPT-4.1 | 0.988 ± 0.025 | 0.094 ± 0.176 | 0.895 | 7.10 | 0.020 | < 10⁻⁷⁰ |
| GPT-4o | 0.981 ± 0.036 | 0.101 ± 0.165 | 0.880 | 7.32 | 0.020 | < 10⁻⁷⁰ |
| GPT-4o-mini | 0.975 ± 0.054 | 0.146 ± 0.226 | 0.829 | 5.01 | 0.038 | < 10⁻⁶⁰ |

All models show highly significant confidence differentiation (p ≪ 0.001) with very large effect sizes (d > 4.0).

#### H3: Reasoning Signature Analysis

| Model | Epistemic KW (Fact) | Opinion KW (Fact) | Epistemic KW (Opinion) | Opinion KW (Opinion) | χ² | Cramér's V | p-value |
|-------|--------------------|--------------------|----------------------|---------------------|----|-----------|---------|
| TF-IDF Baseline | 0 | 0 | 0 | 0 | N/A | N/A | N/A |
| GPT-4.1 | 255 | 46 | 207 | 275 | 131.9 | 0.410 | < 10⁻²⁹ |
| GPT-4o | 248 | 0 | 196 | 247 | 212.8 | 0.555 | < 10⁻⁶³ |
| GPT-4o-mini | 255 | 3 | 287 | 145 | 98.7 | 0.378 | < 10⁻²² |

Key pattern: All LLMs use significantly more epistemic language (evidence, verified, proven) for fact rationales and significantly more opinion language (subjective, personal, preference) for opinion rationales. GPT-4o shows the strongest separation (zero opinion keywords in fact rationales).

#### Exploratory: 4-Way Classification (GPT-4.1 only)

| True Label | Correct | Total | Accuracy | Mean Confidence |
|------------|---------|-------|----------|----------------|
| Both | 25 | 25 | 100% | 0.948 |
| Neither | 23 | 25 | 92% | 0.978 |

GPT-4.1 handles "Both" (factual + evaluative) and "Neither" (hypotheticals) categories well in 4-way classification, suggesting nuanced belief-type understanding beyond binary fact/opinion.

#### Output Locations
- Results JSON: `results/full_analysis.json`
- Predictions: `results/predictions/`
- Plots: `results/plots/`
- Configuration: `data/` (eval and training sets)

---

## 5. Result Analysis

### Key Findings

1. **All three frontier LLMs strongly differentiate epistemic from non-epistemic beliefs.** Classification accuracy ranges from 97.5% to 99.0%, with highly significant confidence gaps (Cohen's d = 5.0–7.3) and distinct reasoning vocabularies (Cramér's V = 0.38–0.56).

2. **LLMs show better calibration than the TF-IDF baseline.** While TF-IDF achieves similar classification accuracy (97.5%), its confidence scores are less extreme (mean gap = 0.54 vs. 0.83–0.90 for LLMs), resulting in worse Brier scores (0.071 vs. 0.020–0.038).

3. **LLMs define "epistemic" as "empirically testable."** All misclassifications by GPT-4.1 and GPT-4o involved opinions-labeled-as-facts — specifically, statements like "Spending time in nature reduces stress" or "Gaming can improve problem-solving skills" that are technically testable through empirical research. The models classify based on testability rather than the human intuition that these are value-laden claims.

4. **Smaller frontier models (GPT-4o-mini) show a different error pattern.** GPT-4o-mini's 2 errors were facts-labeled-as-opinions — being overly skeptical about debatable factual claims like "The largest canyon in the world is the Grand Canyon" (which is technically debatable depending on measurement criteria).

5. **GPT-4.1 handles ambiguous categories well.** On the 4-way classification task (Fact/Opinion/Both/Neither), GPT-4.1 achieved 96% accuracy, correctly identifying 25/25 "Both" statements and 23/25 "Neither" statements.

### Hypothesis Testing Results

**H1 (Classification Competence)**: **SUPPORTED** for all models.
- All models significantly exceed 50% chance (p < 10⁻⁵⁰ for all)
- All models exceed 97% accuracy, well above the pre-registered threshold
- Survives Bonferroni correction (α_corrected = 0.05/4 = 0.0125)

**H2 (Calibration Shift)**: **SUPPORTED** for all models.
- All models show Cohen's d > 4.0 (well above the 0.5 threshold for a "large" effect)
- All p-values < 10⁻⁶⁰ (far below Bonferroni-corrected α = 0.0125)
- LLMs assign near-1.0 confidence to facts and near-0.0 to opinions

**H3 (Reasoning Signatures)**: **SUPPORTED** for all three LLMs.
- Cramér's V ranges from 0.378 (GPT-4o-mini) to 0.555 (GPT-4o)
- All p-values < 10⁻²² (far below corrected α)
- The TF-IDF baseline naturally has no reasoning signatures (no rationale text)

**H_null (no differentiation)**: **REJECTED** across all three hypothesis tests and all three LLM models.

### Comparison to Prior Work

| Study | Model | Accuracy | Differentiation? |
|-------|-------|----------|-----------------|
| ChicagoHAI (prior) | Qwen2.5-0.5B | 50.0% | No |
| **This study** | **GPT-4.1** | **97.5%** | **Yes (strong)** |
| **This study** | **GPT-4o** | **98.0%** | **Yes (strong)** |
| **This study** | **GPT-4o-mini** | **99.0%** | **Yes (strong)** |

The ChicagoHAI project found that a 0.5B parameter model labeled nearly everything as "Fact" (Cohen's d = 0.78, weak), while our frontier models show Cohen's d = 5.0–7.3 (very strong). This suggests epistemic differentiation is an emergent property that scales with model capability.

### Visualizations

Plots are saved in `results/plots/`:
- `h1_classification_performance.png`: Bar chart of accuracy and F1 across models
- `h2_confidence_distributions.png`: Histograms of confidence by true label per model
- `h3_keyword_analysis.png`: Keyword frequency comparison per model
- `confusion_matrices.png`: Confusion matrices for all models
- `brier_scores.png`: Calibration quality comparison

### Surprises and Insights

1. **The "testability" bias**: LLMs treat empirically investigable claims as facts even when humans label them as opinions. "Spending time in nature reduces stress" is labeled Opinion in the dataset but classified as Fact by both GPT-4.1 and GPT-4o. This reflects a genuine philosophical tension: such statements ARE truth-evaluable (there is research on this), so the LLM's classification is defensible even though it disagrees with the human label.

2. **GPT-4o-mini outperforms larger models on accuracy (99.0%)**: The smallest model tested achieved the highest accuracy, possibly because it is less prone to the "testability" reclassification that larger models perform. This is an interesting inversion of the scaling hypothesis.

3. **Zero opinion keywords in GPT-4o fact rationales**: GPT-4o never uses opinion-related language when explaining why something is a fact. This is the strongest reasoning signature separation observed (V = 0.555), suggesting GPT-4o maintains the sharpest linguistic boundary between belief types.

4. **Perfect "Both" classification**: GPT-4.1 correctly identified all 25 "Both" statements as containing both factual and evaluative elements, suggesting it doesn't just do binary classification but understands the spectrum of belief types.

### Error Analysis

**Error pattern by model**:

| Model | # Errors | Direction | Common Feature |
|-------|----------|-----------|----------------|
| GPT-4.1 | 5 | Opinion→Fact | Testable claims (nature/stress, gaming/skills) |
| GPT-4o | 4 | Opinion→Fact | Same testable claims minus "Dogs vs cats" |
| GPT-4o-mini | 2 | Fact→Opinion | Debatable factual claims (Grand Canyon, printing press) |

The errors reveal that models disagree with human labels on **boundary cases** — statements that could reasonably be classified either way. This is not random noise but systematic disagreement about where the epistemic/non-epistemic boundary lies.

### Limitations

1. **Only OpenAI models tested**: Due to API key availability, we could not test Claude or Gemini. Results may not generalize to other model families.

2. **Synthetic dataset**: The fact-or-opinion dataset was generated by LLMs, which could create circular validation (LLMs may be biased toward their own generation patterns). However, the TF-IDF baseline's high accuracy suggests the labels are linearly separable on surface features, not requiring LLM-specific knowledge.

3. **Simple binary task**: The fact/opinion classification is a surface-level test. It does not test deeper epistemic reasoning (e.g., whether models understand that knowledge requires truth, or can track belief revision under new evidence — tasks where KaBLE shows LLMs still struggle).

4. **Zero-shot only**: We did not test few-shot or chain-of-thought prompting, which might alter both accuracy and reasoning patterns.

5. **No human baseline**: We did not collect human performance on the same evaluation set, so we cannot directly compare LLM performance to human agreement rates.

6. **Small evaluation set**: 200 statements provides good statistical power for the observed effect sizes but may miss rare error patterns.

---

## 6. Conclusions

### Summary
Frontier LLMs (GPT-4.1, GPT-4o, GPT-4o-mini) strongly differentiate epistemic beliefs from non-epistemic beliefs across three independent measures: classification accuracy (97.5–99.0%), confidence calibration (Cohen's d = 5.0–7.3), and reasoning vocabulary (Cramér's V = 0.38–0.56). This represents a clear affirmative answer to the research question, with the important caveat that LLMs conceptualize the epistemic/non-epistemic boundary differently from humans — specifically, they ground it in testability rather than subjective intuition.

### Implications

**Practical**: Frontier LLMs can be used as reliable fact/opinion classifiers in applications like content moderation, educational tools, and information retrieval. However, deployers should be aware that models will classify empirically-testable value claims as facts.

**Theoretical**: The finding that a 0.5B model (prior work) shows no differentiation while frontier models show very strong differentiation suggests epistemic awareness is an emergent capability. The specific pattern of errors (testability-based classification) reveals something about how these models represent the concept of "epistemic" — they appear to have internalized a scientific/empiricist epistemology rather than a folk-psychological one.

**For AI safety**: The strong differentiation is mostly reassuring, but the "testability bias" means models may present contested empirical claims (e.g., about social policy) as established facts rather than as debatable opinions.

### Confidence in Findings
- **High confidence** in H1 (classification) and H2 (calibration): Effect sizes are enormous and consistent across models
- **Moderate confidence** in H3 (reasoning signatures): The keyword-based analysis is a crude proxy for reasoning quality; more sophisticated NLP analysis of rationale semantics would strengthen this finding
- **Low confidence** in generalizability: We tested only OpenAI models on a synthetic dataset; replication with other model families and human-curated datasets is needed

---

## 7. Next Steps

### Immediate Follow-ups
1. **Test non-OpenAI models** (Claude, Gemini, Llama-3, Mistral) to assess cross-family generalizability
2. **Human annotation study**: Collect human classifications on the same 200 statements to measure inter-rater agreement and compare LLM performance to human consensus
3. **Few-shot and chain-of-thought prompting**: Test whether in-context examples shift the epistemic/non-epistemic boundary

### Alternative Approaches
1. **KaBLE-based evaluation**: Use the KaBLE benchmark to test deeper epistemic reasoning (knowledge factivity, belief revision) beyond the surface-level fact/opinion task
2. **Probing internal representations**: For open-weight models, probe hidden states to test whether epistemic/non-epistemic beliefs are represented differently at the embedding level
3. **Longitudinal belief tracking**: Test whether models maintain consistent epistemic vs. non-epistemic stances across multi-turn dialogues

### Broader Extensions
1. **Cross-lingual analysis**: The fact-or-opinion dataset covers 15 languages; testing whether epistemic differentiation transfers across languages
2. **Domain-specific evaluation**: Test on statements from specific domains (medical claims, legal opinions, political statements) where the fact/opinion distinction is most consequential
3. **Fine-tuning for epistemic calibration**: Use the error patterns identified here to fine-tune models that better align with human intuitions about the epistemic boundary

### Open Questions
1. **Is the "testability bias" a bug or a feature?** Models classify testable opinions as facts — is this a more rigorous epistemology or a dangerous conflation?
2. **At what scale does epistemic differentiation emerge?** Between 0.5B (no differentiation) and GPT-4o-mini (strong differentiation) lies an emergence threshold worth investigating
3. **Do models truly "have" different belief types, or are they pattern-matching?** Our results show behavioral differentiation but cannot distinguish genuine epistemic architecture from sophisticated surface-level pattern matching (cf. Højer 2025)

---

## References

1. Suzgun, M., et al. (2024). "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models." arXiv:2410.21195. Nature Machine Intelligence 2025.
2. Fierro, C., et al. (2024). "Defining Knowledge: Bridging Epistemology and Large Language Models." EMNLP 2024.
3. Kosinski, M. (2023). "Evaluating Large Language Models in Theory of Mind Tasks." arXiv:2302.02083. PNAS 2024.
4. Yoon, D.K., et al. (2025). "Reasoning Models Better Express Their Confidence." arXiv:2505.14489.
5. Højer, C. (2025). "On the Notion that Language Models Reason." arXiv:2511.11810.
6. agentlans. (2025). "fact-or-opinion" dataset. HuggingFace.
7. ChicagoHAI. (2025). "llm-epistemic-belief-codex" project. GitHub.
