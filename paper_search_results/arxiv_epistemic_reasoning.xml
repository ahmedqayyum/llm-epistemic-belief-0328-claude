<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/vVuHRIzJy1jurYsuLQUnxry60+U</id>
  <title>arXiv Query: search_query=all:epistemic AND all:reasoning AND all:NLP&amp;id_list=&amp;start=0&amp;max_results=20</title>
  <updated>2026-02-25T20:54:17Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:epistemic+AND+(all:reasoning+AND+all:NLP)&amp;start=0&amp;max_results=20&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>20</opensearch:itemsPerPage>
  <opensearch:totalResults>1</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2511.11810v1</id>
    <title>On the Notion that Language Models Reason</title>
    <updated>2025-11-14T19:04:24Z</updated>
    <link href="https://arxiv.org/abs/2511.11810v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.11810v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-14T19:04:24Z</published>
    <arxiv:comment>Accepted at the 1st Workshop on Epistemic Intelligence in Machine Learning, EurIPS 2025</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Bertram HÃ¸jer</name>
    </author>
  </entry>
</feed>
