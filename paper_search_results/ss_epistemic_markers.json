{"total": 375, "offset": 0, "next": 20, "data": [{"paperId": "7b07459092a953165e634629383ab4db2aeaa8cc", "externalIds": {"DBLP": "conf/acl/LiuZ0S25", "ArXiv": "2505.24778", "DOI": "10.48550/arXiv.2505.24778", "CorpusId": 279070559}, "url": "https://www.semanticscholar.org/paper/7b07459092a953165e634629383ab4db2aeaa8cc", "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?", "year": 2025, "citationCount": 10, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.24778, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2329743526", "name": "Jiayu Liu"}, {"authorId": "2256995117", "name": "Qing Zong"}, {"authorId": "1587728690", "name": "Weiqi Wang"}, {"authorId": "2241325169", "name": "Yangqiu Song"}], "abstract": "As large language models (LLMs) are increasingly used in high-stakes domains, accurately assessing their confidence is crucial. Humans typically express confidence through epistemic markers (e.g.,\"fairly confident\") instead of numerical values. However, it remains unclear whether LLMs consistently use these markers to reflect their intrinsic confidence due to the difficulty of quantifying uncertainty associated with various markers. To address this gap, we first define marker confidence as the observed accuracy when a model employs an epistemic marker. We evaluate its stability across multiple question-answering datasets in both in-distribution and out-of-distribution settings for open-source and proprietary LLMs. Our results show that while markers generalize well within the same distribution, their confidence is inconsistent in out-of-distribution scenarios. These findings raise significant concerns about the reliability of epistemic markers for confidence estimation, underscoring the need for improved alignment between marker based confidence and actual model uncertainty. Our code is available at https://github.com/HKUST-KnowComp/MarCon."}, {"paperId": "75309be880e3a2ba8c52b31ea7c826334da986ec", "externalIds": {"DBLP": "journals/corr/abs-2507-06306", "ArXiv": "2507.06306", "DOI": "10.48550/arXiv.2507.06306", "CorpusId": 280137719}, "url": "https://www.semanticscholar.org/paper/75309be880e3a2ba8c52b31ea7c826334da986ec", "title": "Humans overrely on overconfident language models, across languages", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.06306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2362630987", "name": "Neil Rathi"}, {"authorId": "2256674786", "name": "Dan Jurafsky"}, {"authorId": "2310902810", "name": "Kaitlyn Zhou"}], "abstract": "As large language models (LLMs) are deployed globally, it is crucial that their responses are calibrated across languages to accurately convey uncertainty and limitations. Prior work shows that LLMs are linguistically overconfident in English, leading users to overrely on confident generations. However, the usage and interpretation of epistemic markers (e.g.,'I think it's') differs sharply across languages. Here, we study the risks of multilingual linguistic (mis)calibration, overconfidence, and overreliance across five languages to evaluate LLM safety in a global context. Our work finds that overreliance risks are high across languages. We first analyze the distribution of LLM-generated epistemic markers and observe that LLMs are overconfident across languages, frequently generating strengtheners even as part of incorrect responses. Model generations are, however, sensitive to documented cross-linguistic variation in usage: for example, models generate the most markers of uncertainty in Japanese and the most markers of certainty in German and Mandarin. Next, we measure human reliance rates across languages, finding that reliance behaviors differ cross-linguistically: for example, participants are significantly more likely to discount expressions of uncertainty in Japanese than in English (i.e., ignore their'hedging'function and rely on generations that contain them). Taken together, these results indicate a high risk of reliance on overconfident model generations across languages. Our findings highlight the challenges of multilingual linguistic calibration and stress the importance of culturally and linguistically contextualized model safety evaluations."}, {"paperId": "3b6e3cd13c305ad991b1264dc75341ba197893e1", "externalIds": {"ArXiv": "2411.06528", "DBLP": "journals/corr/abs-2411-06528", "DOI": "10.48550/arXiv.2411.06528", "CorpusId": 273963769}, "url": "https://www.semanticscholar.org/paper/3b6e3cd13c305ad991b1264dc75341ba197893e1", "title": "Epistemic Integrity in Large Language Models", "year": 2024, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.06528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2330191808", "name": "Bijean Ghafouri"}, {"authorId": "2281945133", "name": "Shahrad Mohammadzadeh"}, {"authorId": "2330231077", "name": "James Zhou"}, {"authorId": "48761500", "name": "Pratheeksha Nair"}, {"authorId": "2320260830", "name": "Jacob-Junqi Tian"}, {"authorId": "2319350598", "name": "Mayank Goel"}, {"authorId": "2306673886", "name": "Reihaneh Rabbany"}, {"authorId": "2028704969", "name": "J. Godbout"}, {"authorId": "104333826", "name": "Kellin Pelrine"}], "abstract": "Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration $\\unicode{x2013}$ where a model's linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains."}, {"paperId": "217e436fd23fe4184828e02a2b143835d6fd3b28", "externalIds": {"DBLP": "conf/emnlp/ZhouJH23", "ArXiv": "2302.13439", "DOI": "10.18653/v1/2023.emnlp-main.335", "CorpusId": 265150666}, "url": "https://www.semanticscholar.org/paper/217e436fd23fe4184828e02a2b143835d6fd3b28", "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models", "year": 2023, "citationCount": 103, "openAccessPdf": {"url": "https://aclanthology.org/2023.emnlp-main.335.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.13439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "3396547", "name": "Kaitlyn Zhou"}, {"authorId": "2256674786", "name": "Dan Jurafsky"}, {"authorId": "2266400315", "name": "Tatsunori Hashimoto"}], "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like\"I'm sure it's\",\"I think it's\", or\"Wikipedia says it's\"affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty."}, {"paperId": "0a9789328074a8fefb0ff8966639ef2da291facb", "externalIds": {"DBLP": "journals/corr/abs-2410-20774", "ArXiv": "2410.20774", "DOI": "10.48550/arXiv.2410.20774", "CorpusId": 273653843}, "url": "https://www.semanticscholar.org/paper/0a9789328074a8fefb0ff8966639ef2da291facb", "title": "Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation", "year": 2024, "citationCount": 20, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.20774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "1709263", "name": "Dongryeol Lee"}, {"authorId": "1754112065", "name": "Yerin Hwang"}, {"authorId": "2157128089", "name": "Yongi-Mi Kim"}, {"authorId": "2267004352", "name": "Joonsuk Park"}, {"authorId": "2269738216", "name": "Kyomin Jung"}], "abstract": "In line with the principle of honesty, there has been a growing effort to train large language models (LLMs) to generate outputs containing epistemic markers. However, evaluation in the presence of epistemic markers has been largely overlooked, raising a critical question: Could the use of epistemic markers in LLM-generated outputs lead to unintended negative consequences? To address this, we present EMBER, a benchmark designed to assess the robustness of LLM-judges to epistemic markers in both single and pairwise evaluation settings. Our findings, based on evaluations using EMBER, reveal that all tested LLM-judges, including GPT-4o, show a notable lack of robustness in the presence of epistemic markers. Specifically, we observe a negative bias toward epistemic markers, with a stronger bias against markers expressing uncertainty. This suggests that LLM-judges are influenced by the presence of these markers and do not focus solely on the correctness of the content."}, {"paperId": "6b274f0ca6c8dd0dd5a77cfc4a36174f5c8dde12", "externalIds": {"DBLP": "journals/aiethics/Polozov25", "DOI": "10.1007/s43681-025-00806-5", "CorpusId": 280589509}, "url": "https://www.semanticscholar.org/paper/6b274f0ca6c8dd0dd5a77cfc4a36174f5c8dde12", "title": "The deep illusion: a critical analysis of DeepSeek and the limits of large language models", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s43681-025-00806-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s43681-025-00806-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2375538249", "name": "Andrei Polozov"}], "abstract": null}, {"paperId": "8facbe27ab6b0bfee23f2cbe95c77731faabdf17", "externalIds": {"DOI": "10.2139/ssrn.5348251", "CorpusId": 280179020}, "url": "https://www.semanticscholar.org/paper/8facbe27ab6b0bfee23f2cbe95c77731faabdf17", "title": "Protocol Without Prognosis: Clinical Authority in Large-Scale Diagnostic Language Models", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2139/ssrn.5348251?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2139/ssrn.5348251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2362402283", "name": "Agust\u00edn V. Startari"}], "abstract": "This article introduces the concept of syntactic delegation in clinical diagnostic systems. It demonstrates how medical language models issue recommendations without preserving the linguistic markers of clinical uncertainty. The analysis draws from a multilingual corpus of 50,000 radiology reports, balanced across English, Spanish, German, and Mandarin. All data are de-identified and licensed for open research use. Each report is paired with a synthetic rewrite generated by a fine-tuned GPT-4 variant. Two core metrics are introduced. The Hedging Collapse Coefficient (HCC) is defined as 1 \u2212 (h / t), where h represents the number of hedging tokens retained in the model output, and t the total hedging tokens in the source report. The Responsibility Leakage Index (RLI) is defined as d / r, where d is the number of AI-generated decisions executed without clinician sign-off, and r the total number of decisions requiring such sign-off. For the evaluated corpus, mean HCC = 0.47 and mean RLI = 0.22."}, {"paperId": "8c45f5b3aad4ea3929c6e9bf5b173ce545b36d5e", "externalIds": {"DOI": "10.26803/ijlter.24.12.17", "CorpusId": 283623233}, "url": "https://www.semanticscholar.org/paper/8c45f5b3aad4ea3929c6e9bf5b173ce545b36d5e", "title": "Leveraging Large Language Models to Detect Academic Anxiety in Indonesian English for Specific Purposes Students through Reflective Writing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.26803/ijlter.24.12.17?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.26803/ijlter.24.12.17, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2301529427", "name": "Khoirul Anwar"}, {"authorId": "74113081", "name": "B. Harmanto"}], "abstract": "This study investigates the capacity of Large Language Models to identify academic anxiety in reflective writing produced by English for Specific Purposes students from Indonesia. It tackles two main issues: how well LLMs can identify anxiety from linguistic and environmental cues, and how anxiety-related language markers change depending on the type of activity and level of expertise. Employing a quantitative exploratory-correlational design, the study involved 600 undergraduate ESP students from Universitas Muhammadiyah Gresik. In addition to submitting two samples of reflective writing, each participant filled out a validated Academic Anxiety Inventory. To extract important language variables, such as lexical density, emotional Valence, modal usage, and syntactic complexity, transformer-based models (BERT, RoBERTa) were improved. Analytical reflections displayed greater lexical richness and syntactic complexity, but narrative reflections displayed more negative sentiment and hedging, according to MANOVA results, which demonstrated significant differences in anxiety markers. Higher-proficiency students demonstrated balanced rhetorical control and emotional tone, whereas lower-proficiency students exhibited greater signs of language anxiety. These results provide credence to the use of LLMs as non-invasive, scalable instruments for emotional diagnosis in ESP settings."}, {"paperId": "1baa6de184548938e066a647e830a65faa942fc3", "externalIds": {"DBLP": "journals/corr/abs-2601-06032", "ArXiv": "2601.06032", "DOI": "10.48550/arXiv.2601.06032", "CorpusId": 284648764}, "url": "https://www.semanticscholar.org/paper/1baa6de184548938e066a647e830a65faa942fc3", "title": "Applied Theory of Mind and Large Language Models - how good is ChatGPT at solving social vignettes?", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2601.06032, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2204458692", "name": "Anna Katharina Holl-Etten"}, {"authorId": "2403991458", "name": "Nina Schnaderbeck"}, {"authorId": "2403996401", "name": "Elizaveta Kosareva"}, {"authorId": "2403991598", "name": "Leonhard Aron Prattke"}, {"authorId": "2403990175", "name": "Ralph D. Krueger"}, {"authorId": "47649959", "name": "L. Warner"}, {"authorId": "2412275785", "name": "Nora C. Vetter"}], "abstract": "The rapid development of language-based artificial intelligence (AI) offers new possibilities for psychotherapy and assistive systems, particularly benefitting autistic individuals who often respond well to technology. Parents of autistic persons emphasize the importance of appropriate and context-specific communication behavior. This study investigated whether GPT-3.5 Turbo and GPT-4, as language-based AI applications, are fundamentally capable of replicating this type of adequate communication behavior in the form of applied Theory of Mind (ToM). GPT-3.5 Turbo and GPT-4 were evaluated on three established higher-order ToM tasks: the Faux Pas Test, the Social Stories Questionnaire, and the Story Comprehension Test in English and German. Two independent raters scored response accuracy based on standardized manuals. In addition, responses were rated for epistemic markers as indicators of uncertainty. GPT's results were compared to human neurotypical and neurodivergent samples from previous own and others'research. GPT-4 achieved near human accuracy on the Faux Pas Test and outperformed GPT-3.5 Turbo and individuals with autistic traits. On the Social Stories Questionnaire, GPT-4 scored comparable to neurotypical adults, while GPT-3.5 Turbo remained well below. In the Story Comprehension Test, GPT-4 reached scores that exceeded neurotypical adult and adolescent benchmarks. However, GPT-4 used epistemic markers in up to 42% of responses. GPT-4 shows encouraging performance in complex higher-order ToM tasks and may offer future potential as an assistive tool for individuals with (and without) social communication difficulties. Its ability to interpret complex social situations is promising; however, the frequent use of uncertainty markers highlights the need for further study for assistive use and possibly further refinement to ensure consistent and reliable support in real-world use."}, {"paperId": "dc2ae9ecc8a53ca33aac38888a6355f51ee47719", "externalIds": {"DOI": "10.47709/ijmdsa.v5i1.7125", "CorpusId": 285580232}, "url": "https://www.semanticscholar.org/paper/dc2ae9ecc8a53ca33aac38888a6355f51ee47719", "title": "Epistemic Values in Indonesian Language Classroom Interactions: An Analysis of Lecturer Speech Acts at a Politeknik Ganesha Medan", "year": 2026, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.47709/ijmdsa.v5i1.7125?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.47709/ijmdsa.v5i1.7125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2410666501", "name": "Evan Afri"}, {"authorId": "2395708860", "name": "M. Manugeren"}, {"authorId": "2395705606", "name": "Efendi Barus"}], "abstract": "This study investigates the realization of epistemic values in Indonesian language classroom interactions, focusing on how lecturers at Politeknik Ganesha Medan employ speech acts to construct and negotiate knowledge during the teaching process. Adopting a qualitative descriptive approach, the research collected naturally occurring classroom data through observation, recording, and transcription. The analysis draws on Searle\u2019s (1979) speech act theory integrated with an epistemic stance framework to explore how linguistic choices reflect epistemic positioning. The findings reveal that lecturers predominantly use assertive and directive speech acts to deliver information, guide reasoning, and confirm students\u2019 understanding. These speech acts embody epistemic values such as authority, justification, evidence, and shared knowledge construction. Furthermore, the study identifies the use of evidential markers, modal verbs, and hedging devices that indicate varying degrees of certainty and epistemic responsibility. Such linguistic strategies help lecturers manage interpersonal relations, sustain engagement, and promote dialogic learning environments. The study concludes that epistemic values play a crucial role in shaping the quality of interaction and meaning-making in the classroom. Overall, this research contributes to educational linguistics and discourse analysis by demonstrating how epistemic meaning is enacted and negotiated in lecturer discourse within Indonesian tertiary education contexts."}, {"paperId": "8e5e828d7c9aae2e2a77d3b2b02ad3e0c0048f2a", "externalIds": {"DOI": "10.1017/ahsse.2024.17", "CorpusId": 282877909}, "url": "https://www.semanticscholar.org/paper/8e5e828d7c9aae2e2a77d3b2b02ad3e0c0048f2a", "title": "Two Protestant Assemblies in Tianjin: In Search of Markers of Certainty", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1017/ahsse.2024.17?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1017/ahsse.2024.17, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2391443351", "name": "Isabelle Thireau"}], "abstract": "This article is based on fieldwork carried out between 2011 and 2017 at two Protestant places of worship in Tianjin, a city to the southeast of Beijing: an official church and a \u201cdomestic gathering point.\u201d Based on the observation of eighty-three services as well as exchanges with preachers and worshippers, this study was conducted in a context of religious growth, particularly in the case of Protestantism, that saw the multiplication of public gatherings despite restrictions. The analysis shows how, in a context where references to the past cannot be mobilized to make sense of present experiences, preachers and witnesses draw on the Bible and its \u201ctrue stories\u201d to propose a less equivocal understanding of situations encountered in everyday life. In this way, the Bible offers new linguistic models that enable new forms of interpretation, at a distance from the uncertainties but also the ideological rigidities of the language available in twenty-first-century China. It also offers reference points that, freed from notions of doubt, can absorb the expression of all sorts of anxieties and uncertainties. The figure of the \u201cfalse believer\u201d and that of the bad fellow citizen are thus deplored for the same evil: false pretenses. Language, often denounced as misleading and full of dissimulation, is here used to name and contain such suspicions and to develop shared references and interpretations deemed less ambivalent."}, {"paperId": "e4a61e7720a34b7aa65ca8642ddcf7a02e551191", "externalIds": {"DOI": "10.47772/ijriss.2025.91100485", "CorpusId": 284065466}, "url": "https://www.semanticscholar.org/paper/e4a61e7720a34b7aa65ca8642ddcf7a02e551191", "title": "Language of caution and certainty: Writing proficiency in hedges and boosters in student essays", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.47772/ijriss.2025.91100485?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.47772/ijriss.2025.91100485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2400071867", "name": "R. M. C. Lopez"}], "abstract": "The expression of certainty and doubt is crucial in academic writing. Writers and readers must be able to distinguish between subjective evaluation and objective information presented in academic texts. Hedges and boosters primarily serve this function. Despite the evident importance of these devices, there are no apparent studies that measure students\u2019 proficiency in using them. Therefore, this study investigates the proficiency level of students regarding hedges and boosters and examines the two most common grammatical classifications within these features. It further analyzes how students epistemically express their degree of doubt and certainty in their argumentative essays. A corpus of 50 argumentative essays written by students majoring in English Language Studies at a state university in the northern Philippines was analyzed. Overall, the findings suggest that the general proficiency level of the students is in the developing stage, and epistemic modal verbs occur most frequently across all proficiency levels in both classifications of hedges and boosters."}, {"paperId": "837e650fba5d46bc61e1c0465fa6a7adc5a533d1", "externalIds": {"ArXiv": "2405.21028", "DBLP": "journals/corr/abs-2405-21028", "DOI": "10.48550/arXiv.2405.21028", "CorpusId": 270199379}, "url": "https://www.semanticscholar.org/paper/837e650fba5d46bc61e1c0465fa6a7adc5a533d1", "title": "LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models", "year": 2024, "citationCount": 14, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.21028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2281825070", "name": "Elias Stengel-Eskin"}, {"authorId": "2266467463", "name": "Peter Hase"}, {"authorId": "2281826842", "name": "Mohit Bansal"}], "abstract": "When answering questions, LLMs can convey not only an answer, but a level of confidence about the answer being correct. This includes explicit confidence markers (e.g. giving a numeric score) as well as implicit markers, like an authoritative tone or elaborating with additional knowledge. For LLMs to be trustworthy knowledge sources, the confidence they convey should match their actual expertise; however, most current models tend towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. We cast calibration as preference optimization, creating data via a two-agent game, where a speaker model's outputs are judged by a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the resulting models are better calibrated w.r.t. a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better confidence separation between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more and implicitly signals certainty when it is correct by using an authoritative tone or including details. Finally, LACIE finetuning leads to an emergent increase in model abstention (e.g. saying\"I don't know\") for answers that are likely wrong."}, {"paperId": "96e9b45733635cfde4bb998f5ae6436c0c6f3e67", "externalIds": {"DOI": "10.1177/21582440251406416", "CorpusId": 283953083}, "url": "https://www.semanticscholar.org/paper/96e9b45733635cfde4bb998f5ae6436c0c6f3e67", "title": "The Expression of Certainty in Live Streaming: A Comparative Analysis of Epistemic Stance Between Professional Streamers and College Students", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/21582440251406416?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/21582440251406416, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2400071169", "name": "Shuang Wang"}], "abstract": "Epistemic stance is adopted as an approach to convey the speakers\u2019 certainty about knowledge and beliefs. This research aims to make a comprehensive comparison between professional live streamers and Chinese college students in terms of epistemic stance expression in their live streaming. Focusing on three distinct product categories\u2014fashion, food, and home and electronics\u2014the research collected texts from Amazon live streamers\u2019 online broadcasts and Chinese college students\u2019 in-class live streaming training. The investigation employed epistemic stance features derived from Hyland\u2019s model, utilizing 76 stance features categorized as boosters or hedges for analysis. The results revealed that both professional live streamers and college students used boosters significantly more than hedges when expressing epistemic certainty, along with a distinct co-occurrence pattern of self-referential expressions (e.g., I, we) combined with boosters. However, there were statistically significant differences between the two groups in the use of epistemic stance expressions. Professional streamers demonstrated higher frequencies of epistemic devices overall, which was observed in three parts of speech (adverbs, modals, and verbs) and across three product categories. These findings have theoretical significance for advancing study of stance expression and pedagogical implications for business English applied in live streaming training context. Plain Language Summary Expressing Certainty in Live Streaming: How Professional Streamers and College Students Differ in Their Expressions This study compares how professional live streamers and Chinese college students express certainty (epistemic stance) in live streams. It looked at three product categories: fashion, food, and home & electronics. Data came from Amazon streamers\u2019 broadcasts and students\u2019 in-class live stream training. Using Hyland\u2019s model, researchers analyzed 76 features of certainty expression, split into \u201cboosters\u201d (which strengthen certainty) and \u201chedges\u201d (which weaken it). Both groups used boosters much more than hedges. They also often combined self-referential phrases with boosters. But there were big differences: professionals used more certainty-related language overall. This was true for adverbs, modals, and verbs, and across all three product categories. The findings help advance research on how people express certainty and offer tips for teaching business English in live stream training."}, {"paperId": "87a6e84bd1f16ce48c561ef00919c09bff1a7c42", "externalIds": {"DOI": "10.63163/srh143", "CorpusId": 283121957}, "url": "https://www.semanticscholar.org/paper/87a6e84bd1f16ce48c561ef00919c09bff1a7c42", "title": "A COMPARATIVE ANALYSIS OF METADISCOURSE MARKERS IN PAKISTANI AND AMERICAN JOURNALS OF ENGLISH LANGUAGE: A CORPUS-BASED STUDY", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.63163/srh143?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.63163/srh143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2344904999", "name": "Sana Tafseer"}, {"authorId": "2393308907", "name": "Mahnoor Ehsan"}, {"authorId": "2344900731", "name": "Dr. Jahanzeb Jahan"}, {"authorId": "2393303974", "name": "Radiya Rani"}], "abstract": "The present study is a corpus-based comparative analysis of metadiscourse markers with reference to Pakistani and American Journals on English language, based upon Hyland\u2019s (2005) interpersonal model of\u2002metadiscourse. Two corpora were constructed, including 464,349 characters from American journals and 469,399 characters from Pakistani journals along with metadata prepared\u2002in compliance with ethical conduct of research. The frequency and distribution of interactive and interactional\u2002metadiscourse categories, namely transitions, hedges, self-mentions, engagement markers, attitude markers evidential, endophoric markers and frame marker were analyzed using AntConc software.\nThe\u2002findings reveal that metadiscourse is widely used in both corpora to shape reading perceptions and arguments, with some differences between them. American writers exhibited a reader-responsible and dialogic style using many hedging devices (American Journal: 561, Pakistani Journal: 319), engagement\u2002markers and corporate self-references, which was indicative of subtle stance-taking and inclusive communication. Pakistani writers showed writer- based and explicit style, with more additive transitions used\u2002and individual self-references present; clarity, authority and linear policy orientation. These\u2002features indicate culturally determined rhetorical conventions in academic discourse and also highlight the significance of metadiscourse in shaping scholarly identity.\nThe\u2002study has implications for cross-cultural academic discourse, with practical insights for EAP teaching, writing and scholarly pedagogy and editorial practices. The results indicate\u2002that knowledge of metadiscourse can contribute to a pragmatic and intercultural perspective on academic writing."}, {"paperId": "339e557d7f62c8db0c9033cba2981b1345423fff", "externalIds": {"MAG": "3003844252", "DOI": "10.14746/cl.2020.41.3", "CorpusId": 211252924}, "url": "https://www.semanticscholar.org/paper/339e557d7f62c8db0c9033cba2981b1345423fff", "title": "Epistemic Modality: A Corpus-Based Analysis of Epistemic Markers in EU and Polish Judgments", "year": 2019, "citationCount": 6, "openAccessPdf": {"url": "https://doi.org/10.14746/cl.2020.41.3", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14746/cl.2020.41.3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14746/cl.2020.41.3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "1412410719", "name": "Dariusz Ko\u017abia\u0142"}], "abstract": "Abstract The aim of this paper is to establish the repertoire and distribution of verbal and adverbial exponents of epistemic modality in English- and Polish-language judgments passed by the Court of Justice of the EU (CJEU) and non-translated judgments passed by the Supreme Court of Poland (SN). The study applies a model for categorizing exponents of epistemicity with regard to their (i) level (high-, medium- and low-level of certainty, necessity or possibility expressed by the markers; primary dimension), (ii) perspective (own vs. reported perspective), (iii) opinion (based either on facts or beliefs) and (iv) time (the embedding of epistemic markers in sentences relating to the past, present or future) (contextual dimensions). It examines the degree of intra-generic convergence of translated EU judgments and non-translated national judgments in terms of the employment of epistemic markers, as well as the degree of authoritativeness of judicial argumentation, and determines whether the frequent use of epistemic markers constitutes a generic feature of judgments. The research material consists of a parallel corpus of English- and Polish-language versions of 200 EU judgments and a corpus of 200 non-translated domestic judgments. The results point to the high salience and differing patterns of use of epistemic markers in both EU and national judgments. The frequent use of high-level epistemic markers boosts the authoritativeness of judicial reasoning."}, {"paperId": "7a0100a262a85eed06aa66cb41f5b66add9bd7a7", "externalIds": {"MAG": "2891779796", "DOI": "10.18552/JOAW.V8I1.359", "CorpusId": 149520502}, "url": "https://www.semanticscholar.org/paper/7a0100a262a85eed06aa66cb41f5b66add9bd7a7", "title": "Epistemic Markers in NS and NNS Academic Writing", "year": 2018, "citationCount": 5, "openAccessPdf": {"url": "https://publications.coventry.ac.uk/index.php/joaw/article/download/359/593", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18552/JOAW.V8I1.359?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18552/JOAW.V8I1.359, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "66035307", "name": "Tim Vandenhoek"}], "abstract": "The use of epistemic markers \u2013 words and phrases employed to show differing degrees of certainty and hesitation \u2013 is an important element of academic writing. Previous research has suggested that this is an area in which non-native speakers (NNS) struggle. Studies have indicated that NNS are prone to making overly strong statements and exhibit less range and sophistication in the devices that they do employ than native speakers (NS). However, there have as yet been few studies examining the use of such language by advanced NNS students in university academic writing. This corpus-based study examines the ways in which NS and advanced NNS students use epistemic markers in university academic writing. Two corpora totalling 31 659 words were formed from the discussion and conclusion sections of a written university assignment and analyzed for use of epistemic markers. The results indicate that though both groups employ the language at comparable rates and display similar levels of commitment in their writing, these NNS writers display some features of epistemic marker use found to be generally characteristic of other NNS writers."}, {"paperId": "3f730754a6b73513e499022d99bd47e512b3f140", "externalIds": {"DOI": "10.1075/ijchl.20014.wan", "CorpusId": 254394999}, "url": "https://www.semanticscholar.org/paper/3f730754a6b73513e499022d99bd47e512b3f140", "title": "A discourse-pragmatic functional study of Chinese epistemic markers haoxiang \u201cseem\u201d and\n keneng \u201cprobably\u201d", "year": 2022, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1075/ijchl.20014.wan?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1075/ijchl.20014.wan, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2152546046", "name": "Yan Wang"}], "abstract": "\n This study investigates the discourse-pragmatic functions of the epistemic markers haoxiang\n \u201cseem\u201d and keneng \u201cprobably\u201d in natural conversations of Mandarin Chinese. By examining 107 cases of\n haoxiang and 152 cases of keneng in sequential contexts, it demonstrates that both\n haoxiang and keneng are hedge expressions showing the speaker\u2019s attitude of lack of\n commitment to the truthfulness of their own utterance, which is often driven by an intersubjective motivation.\n As epistemic markers, haoxiang tends to mitigate informational certainty that is based on the\n speaker\u2019s personal but vague experience, while keneng is often used to mitigate the assertiveness of the\n speaker\u2019s personal speculation deduced from background knowledge, general knowledge or commonly accepted logic.\n Further, this study claims that both haoxiang and keneng often serve as\n politeness devices to mitigate various Face Threatening Acts (FTAs) such as disconfirmation, disagreement or negative assessment.\n In either case, haoxiang and keneng are not merely epistemic markers revealing the speaker\u2019s\n subjective uncertainty, but also serve as politeness markers for the purpose of intersubjectivity, and their multiple\n discourse-pragmatic usages are rooted in their semantic meanings, respectively."}, {"paperId": "3dbfaaec79e1ef06fe6abd25bbaf5b7f459c5b71", "externalIds": {"DBLP": "journals/corr/abs-2508-08992", "ArXiv": "2508.08992", "DOI": "10.48550/arXiv.2508.08992", "CorpusId": 280635562}, "url": "https://www.semanticscholar.org/paper/3dbfaaec79e1ef06fe6abd25bbaf5b7f459c5b71", "title": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.08992, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2375822021", "name": "Rui Wang"}, {"authorId": "2377534374", "name": "Qihan Lin"}, {"authorId": "2329743526", "name": "Jiayu Liu"}, {"authorId": "2256995117", "name": "Qing Zong"}, {"authorId": "2209990450", "name": "Tianshi ZHENG"}, {"authorId": "1587728690", "name": "Weiqi Wang"}, {"authorId": "2241325169", "name": "Yangqiu Song"}], "abstract": "Prospect Theory (PT) models human decision-making under uncertainty, while epistemic markers (e.g., maybe) serve to express uncertainty in language. However, it remains largely unexplored whether Prospect Theory applies to contemporary Large Language Models and whether epistemic markers, which express human uncertainty, affect their decision-making behaviour. To address these research gaps, we design a three-stage experiment based on economic questionnaires. We propose a more general and precise evaluation framework to model LLMs'decision-making behaviour under PT, introducing uncertainty through the empirical probability values associated with commonly used epistemic markers in comparable contexts. We then incorporate epistemic markers into the evaluation framework based on their corresponding probability values to examine their influence on LLM decision-making behaviours. Our findings suggest that modelling LLMs'decision-making with PT is not consistently reliable, particularly when uncertainty is expressed in diverse linguistic forms. Our code is released in https://github.com/HKUST-KnowComp/MarPT."}, {"paperId": "ca525d3956edf8e96bb18a6864c4c3764e5fe602", "externalIds": {"DOI": "10.61978/lingua.v3i2.996", "CorpusId": 282035180}, "url": "https://www.semanticscholar.org/paper/ca525d3956edf8e96bb18a6864c4c3764e5fe602", "title": "Linguistic Credibility in Digital Academia: The Role of Politeness and Hedging in Peer Endorsed Responses", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.61978/lingua.v3i2.996?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.61978/lingua.v3i2.996, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2265505271", "name": "Sam Hermansyah"}, {"authorId": "147665022", "name": "Nurul Faradillah"}, {"authorId": "69438863", "name": "E. R. Linuwih"}, {"authorId": "2385276383", "name": "Yelnim"}, {"authorId": "116236013", "name": "Mac Aditiawarman"}], "abstract": "Politeness and hedging are central in shaping credibility and interpersonal dynamics in online academic communication. This study examines how these strategies affect persuasion in Q&A forums, particularly Academia and CrossValidated communities on Stack Exchange. It aims to measure their influence on persuasive success through three indicators: answer acceptance, scoring, and response timing. Drawing on a corpus of 20,000+ threads, the study applies computational tools to detect politeness markers and hedging terms. The analysis uses mixed effects logistic regression, negative binomial regression, and Cox proportional hazards models, while controlling for user reputation, message length, and thread depth. Results show that politeness and hedging significantly enhance persuasive outcomes. Posts with more polite and mitigative language are more likely to be accepted, receive upvotes, and get faster responses. The effects are stronger for users with lower reputation, indicating that politeness functions as a compensatory strategy in digital peer interactions. The discussion acknowledges the limits of automated detection tools and stresses the role of context, culture, and disciplinary norms in interpreting politeness and hedging. This study concludes that politeness and hedging are essential rhetorical resources in digital academic dialogue. The findings offer practical implications for AI-driven moderation and feedback systems that aim to support inclusive and effective scholarly communication."}]}
