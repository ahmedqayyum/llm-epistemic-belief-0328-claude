{"total": 864, "offset": 0, "next": 20, "data": [{"paperId": "6875e36ecb05f73d9e5a98729af0b927bb4f94d6", "externalIds": {"DBLP": "conf/icml/LiW0K0LW25", "ArXiv": "2410.16270", "CorpusId": 273507547}, "url": "https://www.semanticscholar.org/paper/6875e36ecb05f73d9e5a98729af0b927bb4f94d6", "title": "Reflection-Bench: Evaluating Epistemic Agency in Large Language Models", "year": 2024, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2308044439", "name": "Lingyu Li"}, {"authorId": "2266363141", "name": "Yixu Wang"}, {"authorId": "2306149464", "name": "Haiquan Zhao"}, {"authorId": "2307914771", "name": "Shuqi Kong"}, {"authorId": "2266238818", "name": "Yan Teng"}, {"authorId": "2239206940", "name": "Chunbo Li"}, {"authorId": "2266364817", "name": "Yingchun Wang"}], "abstract": "With large language models (LLMs) increasingly deployed as cognitive engines for AI agents, the reliability and effectiveness critically hinge on their intrinsic epistemic agency, which remains understudied. Epistemic agency, the ability to flexibly construct, adapt, and monitor beliefs about dynamic environments, represents a base-model-level capacity independent of specific tools, modules, or applications. We characterize the holistic process underlying epistemic agency, which unfolds in seven interrelated dimensions: prediction, decision-making, perception, memory, counterfactual thinking, belief updating, and meta-reflection. Correspondingly, we propose Reflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven tasks with long-term relevance and minimization of data leakage. Through a comprehensive evaluation of 16 models using three prompting strategies, we identify a clear three-tier performance hierarchy and significant limitations of current LLMs, particularly in meta-reflection capabilities. While state-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our findings suggest several promising research directions, including enhancing core cognitive functions, improving cross-functional coordination, and developing adaptive processing mechanisms. Our code and data are available at https://github.com/AI45Lab/ReflectionBench."}, {"paperId": "f2211587c77490d899c33ca5da75803e8d5ff7a0", "externalIds": {"ArXiv": "2504.18085", "DBLP": "journals/corr/abs-2504-18085", "DOI": "10.48550/arXiv.2504.18085", "CorpusId": 278129498}, "url": "https://www.semanticscholar.org/paper/f2211587c77490d899c33ca5da75803e8d5ff7a0", "title": "Random-Set Large Language Models", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2132864456", "name": "Muhammad Mubashar"}, {"authorId": "2170356765", "name": "S. K. Manchingal"}, {"authorId": "2282471567", "name": "Fabio Cuzzolin"}], "abstract": "Large Language Models (LLMs) are known to produce very high-quality tests and responses to our queries. But how much can we trust this generated text? In this paper, we study the problem of uncertainty quantification in LLMs. We propose a novel Random-Set Large Language Model (RSLLM) approach which predicts finite random sets (belief functions) over the token space, rather than probability vectors as in classical LLMs. In order to allow so efficiently, we also present a methodology based on hierarchical clustering to extract and use a budget of\"focal\"subsets of tokens upon which the belief prediction is defined, rather than using all possible collections of tokens, making the method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced in their generation process by the size and diversity of its training set via the size of the credal sets associated with the predicted belief functions. The proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b, Mistral-7b and Phi-2 models and is shown to outperform the standard model in both datasets in terms of correctness of answer while also showing potential in estimating the second level uncertainty in its predictions and providing the capability to detect when its hallucinating."}, {"paperId": "053762ba193ae927d262b5e54f2ff2ef7ff8c5f3", "externalIds": {"ArXiv": "2511.22109", "DBLP": "journals/corr/abs-2511-22109", "DOI": "10.36190/2025.38", "CorpusId": 279256271}, "url": "https://www.semanticscholar.org/paper/053762ba193ae927d262b5e54f2ff2ef7ff8c5f3", "title": "A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large Language Models", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.22109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2344244988", "name": "Gia Bao Hoang"}, {"authorId": "2279845347", "name": "K. Ransom"}, {"authorId": "2366121087", "name": "Rachel G. Stephens"}, {"authorId": "2363711152", "name": "Carolyn Semmler"}, {"authorId": "2268121364", "name": "Nicolas Fay"}, {"authorId": "2344244742", "name": "Lewis Mitchell"}], "abstract": "Traditional psychological models of belief revision focus on face-to-face interactions, but with the rise of social media, more effective models are needed to capture belief revision at scale, in this rich text-based online discourse. Here, we use a hybrid approach, utilizing large language models (LLMs) to develop a model that predicts successful persuasion using features derived from psychological experiments. Our approach leverages LLM generated ratings of features previously examined in the literature to build a random forest classification model that predicts whether a message will result in belief change. Of the eight features tested, \\textit{epistemic emotion} and \\textit{willingness to share} were the top-ranking predictors of belief change in the model. Our findings provide insights into the characteristics of persuasive messages and demonstrate how LLMs can enhance models of successful persuasion based on psychological theory. Given these insights, this work has broader applications in fields such as online influence detection and misinformation mitigation, as well as measuring the effectiveness of online narratives."}, {"paperId": "ff751b2b7ee61e757de5a006e513dbec4e6d31bc", "externalIds": {"DOI": "10.1007/s00146-025-02541-1", "CorpusId": 280728312}, "url": "https://www.semanticscholar.org/paper/ff751b2b7ee61e757de5a006e513dbec4e6d31bc", "title": "Languaging and large language models: a message from radical linguistics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00146-025-02541-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00146-025-02541-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2261904480", "name": "Stephen J. Cowley"}], "abstract": null}, {"paperId": "efddbb7327647c1c13069f85281f479fcfbea564", "externalIds": {"DOI": "10.1215/2834703x-11205161", "CorpusId": 272748164}, "url": "https://www.semanticscholar.org/paper/efddbb7327647c1c13069f85281f479fcfbea564", "title": "What Large Language Models Know", "year": 2024, "citationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1215/2834703x-11205161?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1215/2834703x-11205161, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2266468305", "name": "Rafael C. Alvarado"}], "abstract": "\n The strengths and weaknesses of generative applications built on large language models are by now well-known. They excel at the production of discourse in a variety of genres and styles, from poetry to programs, as well as the combination of these into novel forms. They perform well at high-level question answering, dialogue, and reasoning tasks, suggesting the possession of general intelligence. However, they frequently produce formally correct but factually or logically wrong statements. This essay argues that such failures\u2014so-called hallucinations\u2014are not accidental glitches but are instead a by-product of the design of the transformer architecture on which large language models are built, given its foundation on the distributional hypothesis, a nonreferential approach to meaning. Even when outputs are correct, they do not meet the basic epistemic criterion of justified true belief, suggesting the need to revisit the long neglected relationship between language and reference."}, {"paperId": "248d7baad39026717ad986e762e4e80d1f2db157", "externalIds": {"DBLP": "conf/emnlp/WuXZX25", "ArXiv": "2505.17348", "DOI": "10.48550/arXiv.2505.17348", "CorpusId": 278886743}, "url": "https://www.semanticscholar.org/paper/248d7baad39026717ad986e762e4e80d1f2db157", "title": "DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.17348, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2354175098", "name": "Yuheng Wu"}, {"authorId": "2365267147", "name": "Jianwen Xie"}, {"authorId": "2322818776", "name": "Denghui Zhang"}, {"authorId": "2322457711", "name": "Zhaozhuo Xu"}], "abstract": "Theory-of-Mind (ToM) tasks pose a unique challenge for large language models (LLMs), which often lack the capability for dynamic logical reasoning. In this work, we propose DEL-ToM, a framework that improves verifiable ToM reasoning through inference-time scaling rather than architectural changes. Our approach decomposes ToM tasks into a sequence of belief updates grounded in Dynamic Epistemic Logic (DEL), enabling structured and verifiable dynamic logical reasoning. We use data generated automatically via a DEL simulator to train a verifier, which we call the Process Belief Model (PBM), to score each belief update step. During inference, the PBM evaluates candidate belief traces from the LLM and selects the highest-scoring one. This allows LLMs to allocate extra inference-time compute to yield more transparent reasoning. Experiments across model scales and benchmarks show that DEL-ToM consistently improves performance, demonstrating that verifiable belief supervision significantly enhances LLMs'ToM capabilities without retraining. Code is available at https://github.com/joel-wu/DEL-ToM."}, {"paperId": "332fcaa3961e5d490c28b21a149bc43af70c0483", "externalIds": {"ArXiv": "2602.13832", "CorpusId": 285617083}, "url": "https://www.semanticscholar.org/paper/332fcaa3961e5d490c28b21a149bc43af70c0483", "title": "Beyond Words: Evaluating and Bridging Epistemic Divergence in User-Agent Interaction via Theory of Mind", "year": 2026, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2602.13832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2349811083", "name": "Minyuan Ruan"}, {"authorId": "2307566277", "name": "Ziyue Wang"}, {"authorId": "2276769930", "name": "Kai Liu"}, {"authorId": "2300426001", "name": "Yunghwei Lai"}, {"authorId": "2324893217", "name": "Peng Li"}, {"authorId": "2411242372", "name": "Yang Liu"}], "abstract": "Large Language Models (LLMs) have developed rapidly and are widely applied to both general-purpose and professional tasks to assist human users. However, they still struggle to comprehend and respond to the true user needs when intentions and instructions are imprecisely conveyed, leading to a divergence between subjective user believes and true environment states. Resolving this epistemic divergence requires Theory of Mind (ToM), yet existing ToM evaluations for LLMs primarily focus on isolated belief inference, overlooking its functional utility in real-world interaction. To this end, we formalize ToM for LLMs as a mechanism for epistemic divergence detection and resolution, and propose a benchmark, \\benchname, to assess how models reconcile user beliefs and profiles in practice. Results across 11 leading models reveal a significant limitation to identify underlying cognitive gaps that impede task success. To bridge this gap, we further curate a trajectory-based ToM dataset linking belief tracking with task-related state inference. The model trained on this data via reinforcement learning shows consistent improvement in reasoning about user mental states, leading to enhanced downstream performance. Our work highlights the practical value of ToM as an essential interaction-level mechanism rather than as a standalone reasoning skill."}, {"paperId": "9ce37e116bcf9c04c91456a08fd0ab96ecdc85f5", "externalIds": {"DBLP": "journals/corr/abs-2504-21218", "ArXiv": "2504.21218", "DOI": "10.48550/arXiv.2504.21218", "CorpusId": 278207661}, "url": "https://www.semanticscholar.org/paper/9ce37e116bcf9c04c91456a08fd0ab96ecdc85f5", "title": "Theoretical Foundations for Semantic Cognition in Artificial Intelligence", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.21218, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2358260607", "name": "Sebastian Dumbrava"}], "abstract": "This monograph presents a modular cognitive architecture for artificial intelligence grounded in the formal modeling of belief as structured semantic state. Belief states are defined as dynamic ensembles of linguistic expressions embedded within a navigable manifold, where operators enable assimilation, abstraction, nullification, memory, and introspection. Drawing from philosophy, cognitive science, and neuroscience, we develop a layered framework that enables self-regulating epistemic agents capable of reflective, goal-directed thought. At the core of this framework is the epistemic vacuum: a class of semantically inert cognitive states that serves as the conceptual origin of belief space. From this foundation, the Null Tower arises as a generative structure recursively built through internal representational capacities. The theoretical constructs are designed to be implementable in both symbolic and neural systems, including large language models, hybrid agents, and adaptive memory architectures. This work offers a foundational substrate for constructing agents that reason, remember, and regulate their beliefs in structured, interpretable ways."}, {"paperId": "ba0bdb2accea089e8e5b05a8f1e1ea0d98e777e3", "externalIds": {"DBLP": "journals/corr/abs-2507-09407", "ArXiv": "2507.09407", "DOI": "10.48550/arXiv.2507.09407", "CorpusId": 280294047}, "url": "https://www.semanticscholar.org/paper/ba0bdb2accea089e8e5b05a8f1e1ea0d98e777e3", "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.09407, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2373587377", "name": "Quanyan Zhu"}], "abstract": "We introduce the framework of LLM-Stackelberg games, a class of sequential decision-making models that integrate large language models (LLMs) into strategic interactions between a leader and a follower. Departing from classical Stackelberg assumptions of complete information and rational agents, our formulation allows each agent to reason through structured prompts, generate probabilistic behaviors via LLMs, and adapt their strategies through internal cognition and belief updates. We define two equilibrium concepts: reasoning and behavioral equilibrium, which aligns an agent's internal prompt-based reasoning with observable behavior, and conjectural reasoning equilibrium, which accounts for epistemic uncertainty through parameterized models over an opponent's response. These layered constructs capture bounded rationality, asymmetric information, and meta-cognitive adaptation. We illustrate the framework through a spearphishing case study, where a sender and a recipient engage in a deception game using structured reasoning prompts. This example highlights the cognitive richness and adversarial potential of LLM-mediated interactions. Our results show that LLM-Stackelberg games provide a powerful paradigm for modeling decision-making in domains such as cybersecurity, misinformation, and recommendation systems."}, {"paperId": "2cdcf5850d2919a48605fb8491ad9dad10220620", "externalIds": {"DOI": "10.12775/setf.2026.004", "CorpusId": 285082770}, "url": "https://www.semanticscholar.org/paper/2cdcf5850d2919a48605fb8491ad9dad10220620", "title": "The Epistemological AI Turn: From JTB to KnowledgeS", "year": 2026, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.12775/setf.2026.004?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.12775/setf.2026.004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2343213762", "name": "Roman Krzanowski"}, {"authorId": "2407076412", "name": "Izabela Lipi\u0144ska"}], "abstract": "In this paper, we examine whether large language models (LLMs) can be said to possess knowledge in the sense defined by the Justified True Belief (JTB) framework, and if not, whether any alternative form of knowledge can meaningfully be attributed to them. While LLMs perform impressively across various cognitive tasks\u2014such as summarization, translation, and content generation\u2014they lack belief, justification, and truth-evaluation, which are essential components of the JTB model. We argue that attributing human-like knowledge (in the JTB sense or its variants) to LLMs constitutes a category mistake. Accordingly, LLMs should not be regarded as epistemic agents with human-like capacities, but rather as machine tools that simulate certain functions of human cognition. We acknowledge, however, that when used critically and ethically, these tools can enhance human cognitive performance. To distinguish the capacities of LLMs from human cognitive agency, we introduce the term knowledgeS to denote the structured linguistic outputs produced by LLMs in response to complex cognitive tasks. We refer to the emergence of knowledgeS as marking an \u201cepistemological AI turn.\u201d Finally, we explore the theological implications of AI-generated knowledge. Because LLMs lack conscience and moral sense, they risk detaching knowledge from ethical grounding. Within normative traditions such as Christianity, knowledge is inseparable from moral responsibility rooted in the faith of a religious community. If AI-generated religious texts are mistaken for genuine spiritual insight, they may promote a form of \u201calgorithmic gnosis\u201d\u2014content that mimics sacred language while remaining spiritually hollow. Such developments could erode the moral and spiritual depth of religious expression. As AI systems assume increasingly authoritative roles, society must guard against confusing knowledgeS with genuine, embodied, and ethically accountable knowing, which remains unique to human agency."}, {"paperId": "398d6f3dece71f870628a31c74c33fd2a9d34494", "externalIds": {"CorpusId": 17996673}, "url": "https://www.semanticscholar.org/paper/398d6f3dece71f870628a31c74c33fd2a9d34494", "title": "How Compatiable Are Belief Revision and Mixed Initiative Human-computer Dialogue?", "year": null, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null}, "authors": [{"authorId": "2085067960", "name": "Halpern Hal"}], "abstract": null}, {"paperId": "8afc38e03238de882fbe9cdbd1bc1b2ac9f9d152", "externalIds": {"DBLP": "journals/eaai/FreringSH25", "DOI": "10.1016/j.engappai.2024.109771", "CorpusId": 274748455}, "url": "https://www.semanticscholar.org/paper/8afc38e03238de882fbe9cdbd1bc1b2ac9f9d152", "title": "Integrating Belief-Desire-Intention agents with large language models for reliable human-robot interaction and explainable Artificial Intelligence", "year": 2025, "citationCount": 21, "openAccessPdf": {"url": "https://doi.org/10.1016/j.engappai.2024.109771", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2024.109771?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2024.109771, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2175558353", "name": "Laurent Frering"}, {"authorId": "2135699808", "name": "Gerald Steinbauer-Wagner"}, {"authorId": "2264383273", "name": "Andreas Holzinger"}], "abstract": null}, {"paperId": "7b07459092a953165e634629383ab4db2aeaa8cc", "externalIds": {"DBLP": "conf/acl/LiuZ0S25", "ArXiv": "2505.24778", "DOI": "10.48550/arXiv.2505.24778", "CorpusId": 279070559}, "url": "https://www.semanticscholar.org/paper/7b07459092a953165e634629383ab4db2aeaa8cc", "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?", "year": 2025, "citationCount": 10, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.24778, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2329743526", "name": "Jiayu Liu"}, {"authorId": "2256995117", "name": "Qing Zong"}, {"authorId": "1587728690", "name": "Weiqi Wang"}, {"authorId": "2241325169", "name": "Yangqiu Song"}], "abstract": "As large language models (LLMs) are increasingly used in high-stakes domains, accurately assessing their confidence is crucial. Humans typically express confidence through epistemic markers (e.g.,\"fairly confident\") instead of numerical values. However, it remains unclear whether LLMs consistently use these markers to reflect their intrinsic confidence due to the difficulty of quantifying uncertainty associated with various markers. To address this gap, we first define marker confidence as the observed accuracy when a model employs an epistemic marker. We evaluate its stability across multiple question-answering datasets in both in-distribution and out-of-distribution settings for open-source and proprietary LLMs. Our results show that while markers generalize well within the same distribution, their confidence is inconsistent in out-of-distribution scenarios. These findings raise significant concerns about the reliability of epistemic markers for confidence estimation, underscoring the need for improved alignment between marker based confidence and actual model uncertainty. Our code is available at https://github.com/HKUST-KnowComp/MarCon."}, {"paperId": "33dbdcb057dac3ec84f8160a982227abb6ed19c0", "externalIds": {"ArXiv": "2509.09658", "DBLP": "journals/corr/abs-2509-09658", "DOI": "10.48550/arXiv.2509.09658", "CorpusId": 281252080}, "url": "https://www.semanticscholar.org/paper/33dbdcb057dac3ec84f8160a982227abb6ed19c0", "title": "Measuring Epistemic Humility in Multimodal Large Language Models", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.09658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2372256146", "name": "Bingkui Tong"}, {"authorId": "2182293108", "name": "Jiaer Xia"}, {"authorId": "2362430889", "name": "Sifeng Shang"}, {"authorId": "2364091729", "name": "Kaiyang Zhou"}], "abstract": "Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs'ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a\"None of the above\"option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench."}, {"paperId": "c61e36f520c1e066993e1dc2729e553f7116be27", "externalIds": {"DBLP": "conf/aarhus/ErslevT25", "DOI": "10.1145/3744169.3744195", "CorpusId": 280043117}, "url": "https://www.semanticscholar.org/paper/c61e36f520c1e066993e1dc2729e553f7116be27", "title": "From Bullshit to Cognition: Computing Within the Epistemic Crisis of Large Language Models in Systematic Literature Review", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3744169.3744195?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3744169.3744195, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "1485777857", "name": "M. S. Erslev"}, {"authorId": "1432734195", "name": "T. Tretow-Fish"}], "abstract": "We stipulate that we are in a crisis of epistemology, and that large language models (LLMs) are a central aspect of that crisis. To the end of computing within this crisis, we situate an in-situ experiment with LLMs in a specific, specialized form of research, namely systematic literature review. Contrary to other studies in this domain, we argue that the question of LLM integration in systematic reviews cannot be fully addressed by measuring how well LLMs replicate the labor of human researchers. It is crucial to examine the epistemological implications of how LLMs are integrated into the process. To explore this, we conducted a systematic literature review, situated within the humanities and social sciences, using two different LLM products: a general-purpose model (GPT-4o) and a purpose-specific tool (Elicit). We analyze how our two exemplar implementations influence the review process and what this reveals about the cognitive and epistemological effects of LLMs in research, drawing on a conceptual vocabulary of focusing on notions of bullshit and nonconscious cognition."}, {"paperId": "41be331245fc1c391466c6795e7eb3fad5e67a82", "externalIds": {"ArXiv": "2512.16030", "DBLP": "journals/corr/abs-2512-16030", "DOI": "10.48550/arXiv.2512.16030", "CorpusId": 283934558}, "url": "https://www.semanticscholar.org/paper/41be331245fc1c391466c6795e7eb3fad5e67a82", "title": "Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.16030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2399166330", "name": "Lukas Nel"}], "abstract": "A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\\% confidence, it should be correct 80\\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \\textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \\textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \\emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development."}, {"paperId": "421ff7f6afe2b9a60ee60da51e06142ecd5b6e8a", "externalIds": {"ArXiv": "2510.13103", "DBLP": "journals/corr/abs-2510-13103", "DOI": "10.48550/arXiv.2510.13103", "CorpusId": 282102621}, "url": "https://www.semanticscholar.org/paper/421ff7f6afe2b9a60ee60da51e06142ecd5b6e8a", "title": "ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.13103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2112109549", "name": "Mingda Li"}, {"authorId": "2304362369", "name": "Xinyu Li"}, {"authorId": "2304364922", "name": "Weinan Zhang"}, {"authorId": "2253857109", "name": "Longxuan Ma"}], "abstract": "Uncertainty Quantification (UQ) is a promising approach to improve model reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is non-trivial. In this work, we establish a connection between the uncertainty of LLMs and their invariance under semantic-preserving intervention from a causal perspective. Building on this foundation, we propose a novel grey-box uncertainty quantification method that measures the variation in model outputs before and after the semantic-preserving intervention. Through theoretical justification, we show that our method provides an effective estimate of epistemic uncertainty. Our extensive experiments, conducted across various LLMs and a variety of question-answering (QA) datasets, demonstrate that our method excels not only in terms of effectiveness but also in computational efficiency."}, {"paperId": "3ea2b024108ab5bcf11e5f824acba4be6be0abb1", "externalIds": {"ArXiv": "2506.06153", "DBLP": "journals/corr/abs-2506-06153", "DOI": "10.48550/arXiv.2506.06153", "CorpusId": 279244606}, "url": "https://www.semanticscholar.org/paper/3ea2b024108ab5bcf11e5f824acba4be6be0abb1", "title": "Personalized Large Language Models Can Increase the Belief Accuracy of Social Networks", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.06153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2188162761", "name": "A. Proma"}, {"authorId": "2309244754", "name": "Neeley Pate"}, {"authorId": "2086070326", "name": "Sean Kelty"}, {"authorId": "2408791", "name": "Gourab Ghoshal"}, {"authorId": "2237978400", "name": "J. Druckman"}, {"authorId": "2367328835", "name": "Mohammed E. Hoque"}], "abstract": "Large language models (LLMs) are increasingly involved in shaping public understanding on contested issues. This has led to substantial discussion about the potential of LLMs to reinforce or correct misperceptions. While existing literature documents the impact of LLMs on individuals' beliefs, limited work explores how LLMs affect social networks. We address this gap with a pre-registered experiment (N = 1265) around the 2024 US presidential election, where we empirically explore the impact of personalized LLMs on belief accuracy in the context of social networks. The LLMs are constructed to be personalized, offering messages tailored to individuals' profiles, and to have guardrails for accurate information retrieval. We find that the presence of a personalized LLM leads individuals to update their beliefs towards the truth. More importantly, individuals with a personalized LLM in their social network not only choose to follow it, indicating they would like to obtain information from it in subsequent interactions, but also construct subsequent social networks to include other individuals with beliefs similar to the LLM -- in this case, more accurate beliefs. Therefore, our results show that LLMs have the capacity to influence individual beliefs and the social networks in which people exist, and highlight the potential of LLMs to act as corrective agents in online environments. Our findings can inform future strategies for responsible AI-mediated communication."}, {"paperId": "422b00c330a16a00ef182abfd1d66e12369db9e8", "externalIds": {"DBLP": "journals/corr/abs-2503-15850", "ArXiv": "2503.15850", "DOI": "10.1145/3711896.3736569", "CorpusId": 277150701}, "url": "https://www.semanticscholar.org/paper/422b00c330a16a00ef182abfd1d66e12369db9e8", "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey", "year": 2025, "citationCount": 53, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.15850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2346334761", "name": "Xiaoou Liu"}, {"authorId": "2278853459", "name": "Tiejin Chen"}, {"authorId": "1387492282", "name": "Longchao Da"}, {"authorId": "150946029", "name": "Chacha Chen"}, {"authorId": "2376206674", "name": "Zhen-Yu Lin"}, {"authorId": "2277576540", "name": "Hua Wei"}], "abstract": "Uncertainty quantification (UQ) enhances the reliability of Large Language Models (LLMs) by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions, including input, reasoning, parameter, and prediction uncertainty. We evaluate existing techniques, summarize existing benchmarks and metrics for UQ, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability."}, {"paperId": "c775b5b8504da929766ef021b7c1b291bdce945c", "externalIds": {"DBLP": "journals/corr/abs-2504-05278", "ArXiv": "2504.05278", "DOI": "10.48550/arXiv.2504.05278", "CorpusId": 277626644}, "url": "https://www.semanticscholar.org/paper/c775b5b8504da929766ef021b7c1b291bdce945c", "title": "The challenge of uncertainty quantification of large language models in medicine", "year": 2025, "citationCount": 22, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.05278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2319604548", "name": "Zahra Atf"}, {"authorId": "2115010786", "name": "Seyed Amir Ahmad Safavi-Naini"}, {"authorId": "2351780241", "name": "Peter R. Lewis"}, {"authorId": "2188106320", "name": "Aref Mahjoubfar"}, {"authorId": "2164912732", "name": "Nariman Naderi"}, {"authorId": "2319419712", "name": "Thomas Savage"}, {"authorId": "2249741916", "name": "A. Soroush"}], "abstract": "This study investigates uncertainty quantification in large language models (LLMs) for medical applications, emphasizing both technical innovations and philosophical implications. As LLMs become integral to clinical decision-making, accurately communicating uncertainty is crucial for ensuring reliable, safe, and ethical AI-assisted healthcare. Our research frames uncertainty not as a barrier but as an essential part of knowledge that invites a dynamic and reflective approach to AI design. By integrating advanced probabilistic methods such as Bayesian inference, deep ensembles, and Monte Carlo dropout with linguistic analysis that computes predictive and semantic entropy, we propose a comprehensive framework that manages both epistemic and aleatoric uncertainties. The framework incorporates surrogate modeling to address limitations of proprietary APIs, multi-source data integration for better context, and dynamic calibration via continual and meta-learning. Explainability is embedded through uncertainty maps and confidence metrics to support user trust and clinical interpretability. Our approach supports transparent and ethical decision-making aligned with Responsible and Reflective AI principles. Philosophically, we advocate accepting controlled ambiguity instead of striving for absolute predictability, recognizing the inherent provisionality of medical knowledge."}]}
