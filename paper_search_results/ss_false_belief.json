{"total": 2335, "offset": 0, "next": 20, "data": [{"paperId": "6e02a7eedad079451b9a8dd358268727cf599c6e", "externalIds": {"ArXiv": "2209.01515", "DBLP": "journals/corr/abs-2209-01515", "DOI": "10.48550/arXiv.2209.01515", "CorpusId": 252089182, "PubMed": "37401923"}, "url": "https://www.semanticscholar.org/paper/6e02a7eedad079451b9a8dd358268727cf599c6e", "title": "Do Large Language Models know what humans know?", "year": 2022, "citationCount": 126, "openAccessPdf": {"url": "http://arxiv.org/pdf/2209.01515", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.01515, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "3393311", "name": "Sean Trott"}, {"authorId": "2115254698", "name": "Cameron J. Jones"}, {"authorId": "2087001989", "name": "Tyler A. Chang"}, {"authorId": "118683123", "name": "James A. Michaelov"}, {"authorId": "24316216", "name": "B. Bergen"}], "abstract": "Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre-registered analyses, we present a linguistic version of the False Belief Task to both human participants and a large language model, GPT-3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans nor does it explain the full extent of their behavior-despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how humans develop the ability to reason about the mental states of others, other mechanisms are also\u00a0responsible."}, {"paperId": "26b3bdcaa2e00071549fee2d0c60fbfccff21dcf", "externalIds": {"ArXiv": "2312.07527", "DBLP": "journals/corr/abs-2312-07527", "DOI": "10.48550/arXiv.2312.07527", "CorpusId": 266174592}, "url": "https://www.semanticscholar.org/paper/26b3bdcaa2e00071549fee2d0c60fbfccff21dcf", "title": "BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability", "year": 2023, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.07527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2258709497", "name": "Peter Clark"}, {"authorId": "40135250", "name": "Bhavana Dalvi"}, {"authorId": "3385516", "name": "Oyvind Tafjord"}], "abstract": "While there are numerous benchmarks comparing the performance of modern language models (LMs), end-task evaluations often conflate notions of *factual accuracy* (\"truth\") and *reasoning ability* (\"rationality\", or\"honesty\"in the sense of correctly reporting implications of beliefs). Our goal is a dataset that clearly distinguishes these two notions. Our approach is to leverage and extend a collection of human-annotated *entailment trees*, engineered to express both good and bad chains of reasoning, and using a mixture of true and false facts, in particular including counterfactual examples, to avoid belief bias (also known as the\"content effect\"). The resulting dataset, called BaRDa, contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319 false statements. Testing on four GPT-series models, GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of 74.1/80.6/82.6/87.1 and reasoning accuracy scores of 63.1/78.0/71.8/79.2. This shows the clear progression of models towards improved factual accuracy and entailment reasoning, and the dataset provides a new benchmark that more cleanly separates and quantifies these two notions."}, {"paperId": "8523f1c28ca771a33fe0eef9b05394fb7f02e8ca", "externalIds": {"DBLP": "conf/cmn/KissJ14", "MAG": "2150975940", "DOI": "10.4230/OASIcs.CMN.2014.88", "CorpusId": 16200339}, "url": "https://www.semanticscholar.org/paper/8523f1c28ca771a33fe0eef9b05394fb7f02e8ca", "title": "Mindreading, Privileged Access and Understanding Narratives", "year": 2014, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.4230/OASIcs.CMN.2014.88?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4230/OASIcs.CMN.2014.88, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "11010177", "name": "S. Kiss"}, {"authorId": "35263169", "name": "Z. Jakab"}], "abstract": null}, {"paperId": "6671194eb9913f3276c60a822391834e87216bfd", "externalIds": {"DOI": "10.1038/s42256-025-01113-8", "CorpusId": 282774468}, "url": "https://www.semanticscholar.org/paper/6671194eb9913f3276c60a822391834e87216bfd", "title": "Language models cannot reliably distinguish belief from knowledge and fact", "year": 2025, "citationCount": 10, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s42256-025-01113-8?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s42256-025-01113-8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "51903517", "name": "Mirac Suzgun"}, {"authorId": "2328013164", "name": "Tayfun Gur"}, {"authorId": "2273858095", "name": "Federico Bianchi"}, {"authorId": "2188497728", "name": "Daniel E. Ho"}, {"authorId": "2251009433", "name": "Thomas Icard"}, {"authorId": "2256674786", "name": "Dan Jurafsky"}, {"authorId": "2204356217", "name": "James Y. Zou"}], "abstract": null}, {"paperId": "0de5ba36bc1bf27d2689d661e2de87356eb4d7d4", "externalIds": {"DOI": "10.1038/s42256-025-01145-0", "CorpusId": 283027662}, "url": "https://www.semanticscholar.org/paper/0de5ba36bc1bf27d2689d661e2de87356eb4d7d4", "title": "Large language models still struggle with false beliefs", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s42256-025-01145-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s42256-025-01145-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2392515064", "name": "Kristian Kersting"}], "abstract": null}, {"paperId": "347192591d37037d19b59f98931f420d4a12c9b7", "externalIds": {"ACL": "2024.eacl-long.111", "DBLP": "conf/eacl/ShrawgiRSD24", "CorpusId": 268417107}, "url": "https://www.semanticscholar.org/paper/347192591d37037d19b59f98931f420d4a12c9b7", "title": "Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach", "year": 2024, "citationCount": 39, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/2024.eacl-long.111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2291365186", "name": "Hari Shrawgi"}, {"authorId": "2291363303", "name": "Prasanjit Rath"}, {"authorId": "2949360", "name": "Tushar Singhal"}, {"authorId": "34725175", "name": "Sandipan Dandapat"}], "abstract": null}, {"paperId": "4170fd4ba10eb57157fe98bb015fe8eb260a56e7", "externalIds": {"ArXiv": "2509.08075", "DBLP": "journals/corr/abs-2509-08075", "DOI": "10.48550/arXiv.2509.08075", "CorpusId": 281244007}, "url": "https://www.semanticscholar.org/paper/4170fd4ba10eb57157fe98bb015fe8eb260a56e7", "title": "No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.08075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "1410406981", "name": "F. Plaza-del-Arco"}, {"authorId": "2043232919", "name": "Paul R\u00f6ttger"}, {"authorId": "1742339548", "name": "Nino Scherrer"}, {"authorId": "2287643787", "name": "Emanuele Borgonovo"}, {"authorId": "1766555", "name": "E. Plischke"}, {"authorId": "2267334203", "name": "Dirk Hovy"}], "abstract": "Large language models (LLMs) are increasingly integrated into our daily lives and personalized. However, LLM personalization might also increase unintended side effects. Recent work suggests that persona prompting can lead models to falsely refuse user requests. However, no work has fully quantified the extent of this issue. To address this gap, we measure the impact of 15 sociodemographic personas (based on gender, race, religion, and disability) on false refusal. To control for other factors, we also test 16 different models, 3 tasks (Natural Language Inference, politeness, and offensiveness classification), and nine prompt paraphrases. We propose a Monte Carlo-based method to quantify this issue in a sample-efficient manner. Our results show that as models become more capable, personas impact the refusal rate less and less. Certain sociodemographic personas increase false refusal in some models, which suggests underlying biases in the alignment strategies or safety mechanisms. However, we find that the model choice and task significantly influence false refusals, especially in sensitive content tasks. Our findings suggest that persona effects have been overestimated, and might be due to other factors."}, {"paperId": "dde7c17ccc14c9cd2d93f92e165be53ae3c62725", "externalIds": {"DOI": "10.1101/2024.02.29.582682", "CorpusId": 268252679}, "url": "https://www.semanticscholar.org/paper/dde7c17ccc14c9cd2d93f92e165be53ae3c62725", "title": "Explainable Deep Learning Framework: Decoding Brain Task and Prediction of Individual Performance in False-Belief Task at Early Childhood Stage", "year": 2024, "citationCount": 0, "openAccessPdf": {"url": "https://www.biorxiv.org/content/biorxiv/early/2024/03/04/2024.02.29.582682.full.pdf", "status": "GREEN", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2024.02.29.582682?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2024.02.29.582682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2202425100", "name": "K. Bhavna"}, {"authorId": "2290083514", "name": "Azman Akhter"}, {"authorId": "2290070297", "name": "R. Banerjee"}, {"authorId": "2607391", "name": "Dipanjan Roy"}], "abstract": "Decoding of brain tasks aims to identify individuals\u2019 brain states and brain fingerprints to predict behavior. Deep learning provides an important platform for analyzing brain signals at different developmental stages to understand brain dynamics. Due to their internal architecture and feature extraction techniques, existing machine learning and deep-learning approaches for fMRI-based brain decoding must improve classification performance and explainability. The existing approaches also focus on something other than the behavioral traits that can tell about individuals\u2019 variability in behavioral traits. In the current study, we hypothesized that even at the early childhood stage (as early as 3 years), connectivity between brain regions could decode brain tasks and predict behavioural performance in false-belief tasks. To this end, we proposed an explainable deep learning framework to decode brain states (Theory of Mind and Pain states) and predict individual performance on ToM-related false-belief tasks in a developmental dataset. We proposed an explainable spatiotemporal connectivity-based Graph Convolutional Neural Network (Ex-stGCNN) model for decoding brain tasks. Here, we consider a dataset (age range: 3-12 yrs and adults, samples: 155) in which participants were watching a short, soundless animated movie, \u201dPartly Cloudy,\u201d that activated Theory-of-Mind (ToM) and pain networks. After scanning, the participants underwent a ToMrelated false-belief task, leading to categorization into the pass, fail, and inconsistent groups based on performance. We trained our proposed model using Static Functional Connectivity (SFC) and Inter-Subject Functional Correlations (ISFC) matrices separately. We observed that the stimulus-driven feature set (ISFC) could capture ToM and Pain brain states more accurately with an average accuracy of 94%, whereas it achieved 85% accuracy using SFC matrices. We also validated our results using five-fold cross-validation and achieved an average accuracy of 92%. Besides this study, we applied the SHAP approach to identify neurobiological brain fingerprints that contributed the most to predictions. We hypothesized that ToM network brain connectivity could predict individual performance on false-belief tasks. We proposed an Explainable Convolutional Variational Auto-Encoder model using functional connectivity (FC) to predict individual performance on false-belief tasks and achieved 90% accuracy."}, {"paperId": "18d7eb4b41df1647819cd465047a09201c9aff5e", "externalIds": {"DBLP": "conf/chi/DanryPGE25", "DOI": "10.1145/3706598.3713408", "CorpusId": 278070992}, "url": "https://www.semanticscholar.org/paper/18d7eb4b41df1647819cd465047a09201c9aff5e", "title": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations", "year": 2025, "citationCount": 17, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3706598.3713408?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3706598.3713408, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "1741894876", "name": "Valdemar Danry"}, {"authorId": "24637418", "name": "Pat Pataranutaporn"}, {"authorId": "2239199224", "name": "Matthew Groh"}, {"authorId": "2328484165", "name": "Ziv Epstein"}], "abstract": "Advanced Artificial Intelligence (AI) systems, specifically large language models (LLMs), have the capability to generate not just misinformation, but also deceptive explanations that can justify and propagate false information and discredit true information. We examined the impact of deceptive AI generated explanations on individuals\u2019 beliefs in a pre-registered online experiment with 11,780 observations from 589 participants. We found that in addition to being more persuasive than accurate and honest explanations, AI-generated deceptive explanations can significantly amplify belief in false news headlines and undermine true ones as compared to AI systems that simply classify the headline incorrectly as being true/false. Moreover, our results show that logically invalid explanations are deemed less credible - diminishing the effects of deception. This underscores the importance of teaching logical reasoning and critical thinking skills to identify logically invalid arguments, fostering greater resilience against advanced AI-driven misinformation."}, {"paperId": "a7682669d2819494aac9a7601727ac3077b9dd36", "externalIds": {"ArXiv": "2410.21195", "DBLP": "journals/corr/abs-2410-21195", "DOI": "10.48550/arXiv.2410.21195", "CorpusId": 273655169}, "url": "https://www.semanticscholar.org/paper/a7682669d2819494aac9a7601727ac3077b9dd36", "title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models", "year": 2024, "citationCount": 7, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.21195, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "51903517", "name": "Mirac Suzgun"}, {"authorId": "2328013164", "name": "Tayfun Gur"}, {"authorId": "2273858095", "name": "Federico Bianchi"}, {"authorId": "2282542159", "name": "Daniel E. Ho"}, {"authorId": "2251009433", "name": "Thomas Icard"}, {"authorId": "2256674786", "name": "Dan Jurafsky"}, {"authorId": "2284933280", "name": "James Zou"}], "abstract": "As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to significant consequences in areas such as medical diagnosis, legal judgments, and dissemination of fake news. Despite this, current literature has largely focused on more complex issues such as theory of mind, overlooking more fundamental epistemic challenges. This study systematically evaluates the epistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and Llama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13 tasks. Our results reveal key limitations. First, while LMs achieve 86% accuracy on factual scenarios, their performance drops significantly with false scenarios, particularly in belief-related tasks. Second, LMs struggle with recognizing and affirming personal beliefs, especially when those beliefs contradict factual data, which raises concerns for applications in healthcare and counseling, where engaging with a person's beliefs is critical. Third, we identify a salient bias in how LMs process first-person versus third-person beliefs, performing better on third-person tasks (80.7%) compared to first-person tasks (54.4%). Fourth, LMs lack a robust understanding of the factive nature of knowledge, namely, that knowledge inherently requires truth. Fifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the deeper reasoning. These findings highlight significant concerns about current LMs' ability to reason about truth, belief, and knowledge while emphasizing the need for advancements in these areas before broad deployment in critical sectors."}, {"paperId": "80ee1aabd922e849a1180e8f620be42022a99572", "externalIds": {"DBLP": "journals/fi/RoumeliotisTN25", "DOI": "10.3390/fi17010028", "CorpusId": 275472164}, "url": "https://www.semanticscholar.org/paper/80ee1aabd922e849a1180e8f620be42022a99572", "title": "Fake News Detection and Classification: A Comparative Study of Convolutional Neural Networks, Large Language Models, and Natural Language Processing Models", "year": 2025, "citationCount": 28, "openAccessPdf": {"url": "https://doi.org/10.3390/fi17010028", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fi17010028?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fi17010028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2090274411", "name": "Konstantinos I. Roumeliotis"}, {"authorId": "2262352238", "name": "Nikolaos D. Tselikas"}, {"authorId": "70411656", "name": "Dimitrios K. Nasiopoulos"}], "abstract": "In an era where fake news detection has become a pressing issue due to its profound impacts on public opinion, democracy, and social trust, accurately identifying and classifying false information is a critical challenge. In this study, the effectiveness is investigated of advanced machine learning models\u2014convolutional neural networks (CNNs), bidirectional encoder representations from transformers (BERT), and generative pre-trained transformers (GPTs)\u2014for robust fake news classification. Each model brings unique strengths to the task, from CNNs\u2019 pattern recognition capabilities to BERT and GPTs\u2019 contextual understanding in the embedding space. Our results demonstrate that the fine-tuned GPT-4 Omni models achieve 98.6% accuracy, significantly outperforming traditional models like CNNs, which achieved only 58.6%. Notably, the smaller GPT-4o mini model performed comparably to its larger counterpart, highlighting the cost-effectiveness of smaller models for specialized tasks. These findings emphasize the importance of fine-tuning large language models (LLMs) to optimize the performance for complex tasks such as fake news classifier development, where capturing subtle contextual relationships in text is crucial. However, challenges such as computational costs and suboptimal outcomes in zero-shot classification persist, particularly when distinguishing fake content from legitimate information. By highlighting the practical application of fine-tuned LLMs and exploring the potential of few-shot learning for fake news detection, this research provides valuable insights for news organizations seeking to implement scalable and accurate solutions. Ultimately, this work contributes to fostering transparency and integrity in journalism through innovative AI-driven methods for fake news classification and automated fake news classifier systems."}, {"paperId": "108bc0498629f4710b44076fe0c6270954494097", "externalIds": {"DBLP": "conf/ranlp/AdewumiHAB25", "ArXiv": "2404.04631", "DOI": "10.48550/arXiv.2404.04631", "CorpusId": 269005422}, "url": "https://www.semanticscholar.org/paper/108bc0498629f4710b44076fe0c6270954494097", "title": "On the Limitations of Large Language Models (LLMs): False Attribution", "year": 2024, "citationCount": 13, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.04631, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "51221489", "name": "Tosin P. Adewumi"}, {"authorId": "2282137699", "name": "Nudrat Habib"}, {"authorId": "2056905102", "name": "Lama Alkhaled"}, {"authorId": "2274935712", "name": "E. Barney"}], "abstract": "In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models."}, {"paperId": "3cbf4a3852eddd15fd296dc19057a42c81425c38", "externalIds": {"DOI": "10.1002/icd.2528", "CorpusId": 271393933}, "url": "https://www.semanticscholar.org/paper/3cbf4a3852eddd15fd296dc19057a42c81425c38", "title": "Beyond the false belief task: How children develop their knowledge about the mind", "year": 2024, "citationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/icd.2528?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/icd.2528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2248678078", "name": "Marta Bia\u0142ecka"}, {"authorId": "46668258", "name": "A. Gut"}, {"authorId": "1460038190", "name": "Ma\u0142gorzata St\u0119pie\u0144-Nycz"}, {"authorId": "2093003906", "name": "Krystian Macheta"}, {"authorId": "2186704309", "name": "Jakub Janczura"}], "abstract": "Previous research on children's knowledge about the mind has primarily focused on their comprehension of false beliefs, leaving the conceptualization of thoughts and thinking less explored. To address this gap, we developed a new assessment tool, the interview about the mind (IaM), to assess children's understanding of the mind. Two studies involving Polish preschool and early school\u2010aged children were conducted to assess the validity and reliability of the tool: a cross\u2010sectional study with 212 preschoolers aged three to 6\u2009years (106 boys, M\u2009=\u200959\u2009months) followed by a longitudinal study with approximately 200 children (110 boys) assessed at ages 5.5, 6.5, and 7.5\u2009years. We found that the IaM possesses robust psychometric properties and a one\u2010factor structure. Younger children recognize that thoughts are imperceptible to the senses and identified the mind's location as the head. Older children exhibited difficulties with analogical or abstract definitions of thoughts. As expected, a linear developmental trajectory was evident from ages 5.5\u20137.5, with children's language abilities showing a positive correlation with IaM scores. Future research should explore the relationship between knowledge about the mind and theory of mind (ToM), as well as cross\u2010cultural differences in children's mind conceptualization."}, {"paperId": "3df3dffd436b1145b9eee72754fe06e949250774", "externalIds": {"MAG": "2811154434", "DOI": "10.1016/j.cognition.2018.06.017", "CorpusId": 51599061, "PubMed": "29981965"}, "url": "https://www.semanticscholar.org/paper/3df3dffd436b1145b9eee72754fe06e949250774", "title": "The relationship between parental mental-state language and 2.5-year-olds\u2019 performance on a nontraditional false-belief task", "year": 2018, "citationCount": 20, "openAccessPdf": {"url": "https://escholarship.org/content/qt99z4s32h/qt99z4s32h.pdf?t=pc4cqp", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.cognition.2018.06.017?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.cognition.2018.06.017, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "6892274", "name": "Erin Roby"}, {"authorId": "37432883", "name": "Rose M. Scott"}], "abstract": null}, {"paperId": "dce4e4cf1cd8c42b8a400280a48283234ad7aafb", "externalIds": {"DBLP": "conf/sigdial/HudecekD23", "ArXiv": "2304.06556", "ACL": "2023.sigdial-1.21", "DOI": "10.18653/v1/2023.sigdial-1.21", "CorpusId": 258108409}, "url": "https://www.semanticscholar.org/paper/dce4e4cf1cd8c42b8a400280a48283234ad7aafb", "title": "Are Large Language Models All You Need for Task-Oriented Dialogue?", "year": 2023, "citationCount": 79, "openAccessPdf": {"url": "https://aclanthology.org/2023.sigdial-1.21.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.06556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2129445909", "name": "Vojtvech Hudevcek"}, {"authorId": "2544049", "name": "Ondrej Dusek"}], "abstract": "Instruction-finetuned large language models (LLMs) gained a huge popularity recently, thanks to their ability to interact with users through conversation. In this work, we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that in explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show some ability to guide the dialogue to a successful ending through their generated responses if they are provided with correct slot values. Furthermore, this ability improves with few-shot in-domain examples."}, {"paperId": "0eaf243f2f7c8a381baf0952f85396e2f6a655c5", "externalIds": {"DBLP": "journals/corr/abs-2409-12822", "ArXiv": "2409.12822", "DOI": "10.48550/arXiv.2409.12822", "CorpusId": 272753153}, "url": "https://www.semanticscholar.org/paper/0eaf243f2f7c8a381baf0952f85396e2f6a655c5", "title": "Language Models Learn to Mislead Humans via RLHF", "year": 2024, "citationCount": 86, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.12822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2104586007", "name": "Jiaxin Wen"}, {"authorId": "2305484278", "name": "Ruiqi Zhong"}, {"authorId": "2321890630", "name": "Akbir Khan"}, {"authorId": "2261084752", "name": "Ethan Perez"}, {"authorId": "2269733338", "name": "Jacob Steinhardt"}, {"authorId": "2254009342", "name": "Minlie Huang"}, {"authorId": "2297768298", "name": "Samuel R. Bowman"}, {"authorId": "2321875898", "name": "He He"}, {"authorId": "2297816489", "name": "Shi Feng"}], "abstract": "Language models (LMs) can produce errors that are hard to detect for humans, especially when the task is complex. RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at convincing humans that they are right even when they are wrong. We study this phenomenon under a standard RLHF pipeline, calling it\"U-SOPHISTRY\"since it is Unintended by model developers. Specifically, we ask time-constrained (e.g., 3-10 minutes) human subjects to evaluate the correctness of model outputs and calculate humans' accuracy against gold labels. On a question-answering task (QuALITY) and programming task (APPS), RLHF makes LMs better at convincing our subjects but not at completing the task correctly. RLHF also makes the model harder to evaluate: our subjects' false positive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, we show that probing, a state-of-the-art approach for detecting Intended Sophistry (e.g. backdoored LMs), does not generalize to U-SOPHISTRY. Our results highlight an important failure mode of RLHF and call for more research in assisting humans to align them."}, {"paperId": "bcbb02a484ca501dcedd8bc6be0c56cf69e42b2a", "externalIds": {"DBLP": "journals/cbsn/MarchettiM0GM25", "DOI": "10.1089/cyber.2024.0536", "CorpusId": 278391897, "PubMed": "40333375"}, "url": "https://www.semanticscholar.org/paper/bcbb02a484ca501dcedd8bc6be0c56cf69e42b2a", "title": "Artificial Intelligence and the Illusion of Understanding: A Systematic Review of Theory of Mind and Large Language Models", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1089/cyber.2024.0536?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1089/cyber.2024.0536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2312398690", "name": "Antonella Marchetti"}, {"authorId": "31636934", "name": "F. Manzi"}, {"authorId": "2279720491", "name": "Giuseppe Riva"}, {"authorId": "1700503", "name": "A. Gaggioli"}, {"authorId": "4623295", "name": "D. Massaro"}], "abstract": "The development of Large Language Models (LLMs) has sparked significant debate regarding their capacity for Theory of Mind (ToM)\u2014the ability to attribute mental states to oneself and others. This systematic review examines the extent to which LLMs exhibit Artificial ToM (AToM) by evaluating their performance on ToM tasks and comparing it with human responses. While LLMs, particularly GPT-4, perform well on first-order false belief tasks, they struggle with more complex reasoning, such as second-order beliefs and recursive inferences, where humans consistently outperform them. Moreover, the review underscores the variability in ToM assessments, as many studies adapt classical tasks for LLMs, raising concerns about comparability with human ToM. Most evaluations remain constrained to text-based tasks, overlooking embodied and multimodal dimensions crucial to human social cognition. This review discusses the \u201cillusion of understanding\u201d in LLMs for two primary reasons: First, their lack of the developmental and cognitive mechanisms necessary for genuine ToM, and second, methodological biases in test designs that favor LLMs\u2019 strengths, limiting direct comparisons with human performance. The findings highlight the need for more ecologically valid assessments and interdisciplinary research to better delineate the limitations and potential of AToM. This set of issues is highly relevant to psychology, as language is generally considered just one component in the broader development of human ToM, a perspective that contrasts with the dominant approach in AToM studies. This discrepancy raises critical questions about the extent to which human ToM and AToM are comparable."}, {"paperId": "aea7b50352d78fd1c716181a75f086744b7fcd8d", "externalIds": {"ArXiv": "2511.16035", "DBLP": "journals/corr/abs-2511-16035", "DOI": "10.48550/arXiv.2511.16035", "CorpusId": 283110123}, "url": "https://www.semanticscholar.org/paper/aea7b50352d78fd1c716181a75f086744b7fcd8d", "title": "Liars' Bench: Evaluating Lie Detectors for Language Models", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.16035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2393208173", "name": "Kieron Kretschmar"}, {"authorId": "2313479654", "name": "Walter Laurito"}, {"authorId": "2313478753", "name": "Sharan Maiya"}, {"authorId": "2383168275", "name": "Samuel Marks"}], "abstract": "Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generate statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS'BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS'BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS'BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection."}, {"paperId": "b241cc3a1502039724c9a2a3013da3b509e472ed", "externalIds": {"DBLP": "journals/corr/abs-2601-08668", "ArXiv": "2601.08668", "DOI": "10.48550/arXiv.2601.08668", "CorpusId": 284704398}, "url": "https://www.semanticscholar.org/paper/b241cc3a1502039724c9a2a3013da3b509e472ed", "title": "Analyzing Bias in False Refusal Behavior of Large Language Models for Hate Speech Detoxification", "year": 2026, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2601.08668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "2356526340", "name": "Kyuri Im"}, {"authorId": "2201687496", "name": "Shuzhou Yuan"}, {"authorId": "2281825175", "name": "Michael Farber"}], "abstract": "While large language models (LLMs) have increasingly been applied to hate speech detoxification, the prompts often trigger safety alerts, causing LLMs to refuse the task. In this study, we systematically investigate false refusal behavior in hate speech detoxification and analyze the contextual and linguistic biases that trigger such refusals. We evaluate nine LLMs on both English and multilingual datasets, our results show that LLMs disproportionately refuse inputs with higher semantic toxicity and those targeting specific groups, particularly nationality, religion, and political ideology. Although multilingual datasets exhibit lower overall false refusal rates than English datasets, models still display systematic, language-dependent biases toward certain targets. Based on these findings, we propose a simple cross-translation strategy, translating English hate speech into Chinese for detoxification and back, which substantially reduces false refusals while preserving the original content, providing an effective and lightweight mitigation approach."}, {"paperId": "93c30e8832ffde1c1c65276ce6e51bc35087cdf4", "externalIds": {"DBLP": "journals/corr/abs-2503-01659", "ArXiv": "2503.01659", "DOI": "10.48550/arXiv.2503.01659", "CorpusId": 276773353}, "url": "https://www.semanticscholar.org/paper/93c30e8832ffde1c1c65276ce6e51bc35087cdf4", "title": "Detecting Stylistic Fingerprints of Large Language Models", "year": 2025, "citationCount": 7, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "authors": [{"authorId": "1405506735", "name": "Yehonatan Bitton"}, {"authorId": "2348468222", "name": "Elad Bitton"}, {"authorId": "2348465980", "name": "Shai Nisan"}], "abstract": "Large language models (LLMs) have distinct and consistent stylistic fingerprints, even when prompted to write in different writing styles. Detecting these fingerprints is important for many reasons, among them protecting intellectual property, ensuring transparency regarding AI-generated content, and preventing the misuse of AI technologies. In this paper, we present a novel method to classify texts based on the stylistic fingerprints of the models that generated them. We introduce an LLM-detection ensemble that is composed of three classifiers with varied architectures and training data. This ensemble is trained to classify texts generated by four well-known LLM families: Claude, Gemini, Llama, and OpenAI. As this task is highly cost-sensitive and might have severe implications, we want to minimize false-positives and increase confidence. We consider a prediction as valid when all three classifiers in the ensemble unanimously agree on the output classification. Our ensemble is validated on a test set of texts generated by Claude, Gemini, Llama, and OpenAI models, and achieves extremely high precision (0.9988) and a very low false-positive rate (0.0004). Furthermore, we demonstrate the ensemble's ability to distinguish between texts generated by seen and unseen models. This reveals interesting stylistic relationships between models. This approach to stylistic analysis has implications for verifying the originality of AI-generated texts and tracking the origins of model training techniques."}]}
